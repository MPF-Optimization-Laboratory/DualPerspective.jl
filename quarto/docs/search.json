[
  {
    "objectID": "derivations.html",
    "href": "derivations.html",
    "title": "Cross Entropy under Linear Constraints",
    "section": "",
    "text": "Define the cross-entropy and log-exponential functions by \\[\n\\begin{aligned}\n\\mathop{\\mathrm{cent}}(p \\mid q) &= \\sum_{i=1}^n p_i \\log(p_i/q_i), \\\\\n\\mathop{\\mathrm{logexp}}(p \\mid q) &= \\log\\sum_{i=1}^n q_i\\exp(p_i),\n\\end{aligned}\n\\] which form a conjugate pair. We consider the pair of dual optimization problems \\[\n\\begin{aligned}\n\\min_{p,\\, y } &\\enspace\\left\\{\\mathop{\\mathrm{cent}}(p \\mid q) + \\lambda/2\\|y\\|^2 \\mid Ax + \\lambda y = b\\right\\}\\\\\n\\min_{y} &\\enspace\\left\\{\\mathop{\\mathrm{logexp}}(A^Ty \\mid q) + \\lambda/2\\|y\\|^2 - \\langle b,y \\rangle\\right\\},\n\\end{aligned}\n\\] where \\(A\\) is an \\(m\\)-by-\\(n\\) matrix, \\(b\\) is an \\(m\\)-vector, and \\(\\lambda\\) is a nonnegative regularization parameter."
  },
  {
    "objectID": "derivations.html#problem-statement",
    "href": "derivations.html#problem-statement",
    "title": "Cross Entropy under Linear Constraints",
    "section": "",
    "text": "Define the cross-entropy and log-exponential functions by \\[\n\\begin{aligned}\n\\mathop{\\mathrm{cent}}(p \\mid q) &= \\sum_{i=1}^n p_i \\log(p_i/q_i), \\\\\n\\mathop{\\mathrm{logexp}}(p \\mid q) &= \\log\\sum_{i=1}^n q_i\\exp(p_i),\n\\end{aligned}\n\\] which form a conjugate pair. We consider the pair of dual optimization problems \\[\n\\begin{aligned}\n\\min_{p,\\, y } &\\enspace\\left\\{\\mathop{\\mathrm{cent}}(p \\mid q) + \\lambda/2\\|y\\|^2 \\mid Ax + \\lambda y = b\\right\\}\\\\\n\\min_{y} &\\enspace\\left\\{\\mathop{\\mathrm{logexp}}(A^Ty \\mid q) + \\lambda/2\\|y\\|^2 - \\langle b,y \\rangle\\right\\},\n\\end{aligned}\n\\] where \\(A\\) is an \\(m\\)-by-\\(n\\) matrix, \\(b\\) is an \\(m\\)-vector, and \\(\\lambda\\) is a nonnegative regularization parameter."
  },
  {
    "objectID": "derivations.html#implicit-approach",
    "href": "derivations.html#implicit-approach",
    "title": "Cross Entropy under Linear Constraints",
    "section": "Implicit approach",
    "text": "Implicit approach\n\nOptimality conditions\nA pair \\((p, y)\\) is primal-dual optimal if and only if it satisfies the optimality conditions \\[\n\\begin{aligned}\nb &= Ax + \\lambda y, \\\\\nx_j &= \\frac{1}{\\sigma(x)}{q_j\\exp(\\langle a_j,y \\rangle)}, \\quad j\\in[n],\\\\\n\\sigma(x) &= \\sum_{j=1}^n q_j\\exp(\\langle a_j,y \\rangle),\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Maximum Entropy Linear Inversion Project",
    "section": "",
    "text": "synthetic data"
  },
  {
    "objectID": "index.html#test-sets",
    "href": "index.html#test-sets",
    "title": "Maximum Entropy Linear Inversion Project",
    "section": "",
    "text": "synthetic data"
  },
  {
    "objectID": "synthetic-data.html",
    "href": "synthetic-data.html",
    "title": "KL-Regularized Least Squares",
    "section": "",
    "text": "Import packages\nusing Revise\nusing KLLS\nusing NPZ\nusing Plots\nusing Printf\nusing UnPack\nusing LinearAlgebra"
  },
  {
    "objectID": "synthetic-data.html#data-sets",
    "href": "synthetic-data.html#data-sets",
    "title": "KL-Regularized Least Squares",
    "section": "Data sets",
    "text": "Data sets\nThere are three data sets available:\n\nPhysicsData.npz\nsynthetic_data.npz\nTobias_data.npz\n\n\n\n\n\n\n\nWarning\n\n\n\nThe Tobias_data.npz file doesn’t seem to be any different from synthetic_data.npz.\n@TC or @Tobias: please provide more information on which data set we should use? For now, we’ll focus only on the first two data sets.\n\n\n\nPhysicsData:\n\ndataPhy = npzread(\"../data/PhysicsData.npz\", [\"A\", \"b\", \"x0\"])\nprobPhy = KLLSData(dataPhy[\"A\"], dataPhy[\"b\"], name=\"Physics\")\n\nKLLS data: Physics\nm   =    50   norm(b)    =  6.18e+00\nn   =   400   sum(b)     =  8.65e+00\n\n\nCheck normalization:\n\nsum(dataPhy[\"x0\"])\n\n1.0\n\n\n\n\nSynthetic data:\nFor this dataset, the ground truth seems to be named x instead of x0, as in the other data sets. We’ll rename it for consistency:\n\ndataSyn = npzread(\"../data/synthetic_data.npz\", [\"A\", \"b\", \"x\"])\ndataSyn[\"x0\"] = dataSyn[\"x\"] # convenient\nprobSyn = KLLSData(dataSyn[\"A\"], dataSyn[\"b\"], name=\"Synthetic Data\")\n\nKLLS data: Synthetic Data\nm   =   201   norm(b)    =  2.93e+01\nn   =   250   sum(b)     =  3.25e+02\n\n\nCheck normalization:\n\nsum(dataSyn[\"x0\"])\n\n6.3280697f0\n\n\n\n\n\n\n\n\nNormalization\n\n\n\nThe synthetic data doesn’t appear to be normalized. As agreed, we normalize the data such that the first element of the right-hand side vector \\(b\\) is 1:\n\nprobSyn.b ./= probSyn.b[1]\nprobSyn.A ./= probSyn.b[1]\ndataSyn[\"x0\"] ./= probSyn.b[1]\n\nQuestions:\n\nNote that this normalization still doesn’t put x0 into the simplex:\n\n\nsum(dataSyn[\"x0\"])\n\n6.3280697f0\n\n\n\nWhy is x0 for this test set stored as single precision float? (See “f” in line above.)\n\n\n\nHere are the signals to be recovered:\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecovery\nRecover the signals as the solution of the KL-regularized least squares problem \\[\n\\min_p \\frac{1}{2} \\|Ap - b\\|^2 + \\lambda \\sum_{i=1}^n p_i \\log(p_i)\n\\]\n\n\nImplement test functions\nlogrange(start, stop, length) = exp10.(range(start, stop=stop, length=length))\nsolve_range(prob, λs) = map(λs) do λ\n        prob.λ = λ\n        p, y, stats = newtoncg(prob)\n        (λ=λ, p=p, iters=stats.iter, solve_time=stats.elapsed_time)\nend\nfunction plot_results(stats, x0; title=\"Recovered distributions\")\n    lab = hcat([@sprintf(\"λ = %6.0e\", λ) for λ in getfield.(stats, :λ)]...)\n    default(lw=2, title=title)\n    plot(getfield.(stats, :p), label=lab)\n    plot!(x0, label=\"ground truth\")\nend\n\n\n\nPhysics data\nSolve the problem for a range of logarithmically spaced regularization parameters \\(\\lambda\\) between \\(10^{-16}\\) and \\(10^{-5}\\).\n\nstatsPhy = solve_range(probPhy, logrange(-16, -5, 3))\nplot_results(statsPhy, dataPhy[\"x0\"])\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReport results for smallest value of \\(\\lambda\\):\n\n\nImplement report function\nfunction report_errors(stats, data)\n  @unpack A, b, x0 = data\n  p = stats.p\n  m, n = size(A)\n  @printf(\"%20s: %11.2e\\n\", \"rms(p-x0)\", norm(p - x0)/√n)\n  @printf(\"%20s: %11.2e\\n\", \"rms(Ap-b)\", norm(A*p - b)/√m)\n  @printf(\"%20s: %11.2e\\n\", \"rms(Ax0-b)\", norm(A*x0 - b)/√m)\n  @printf(\"%20s: %11f\\n\", \"Solve time (sec)\", stats.solve_time)\nend\n\n\n\nreport_errors(statsPhy[1], dataPhy)\n\n           rms(p-x0):    4.57e-04\n           rms(Ap-b):    1.27e-08\n          rms(Ax0-b):    2.55e-16\n    Solve time (sec):    0.014566\n\n\n\n\nSynthetic data\nHere we try to recover the solution from the synthetic data set. But as we see here, we get garbage:\n\np, y, statsSyn = newtoncg(probSyn)\nplot([p, dataSyn[\"x0\"]], label=[\"Recovered\" \"Ground truth\"])\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIt seems that the vector b in that data set isn’t correct. What is b the meaning of b in this data set?\n\n\nAs a test, we construct b from the ground truth (forcibly normalized) x0:\n\nlet\n  @unpack A, x0 = dataSyn\n  x0 = x0/sum(x0)\n  b = A*x0\n  p, y, stats = newtoncg(KLLSData(A, b))\n  plot([p, x0], label=[\"Recovered\" \"Ground truth\"])\nend"
  }
]