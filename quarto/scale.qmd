---
title: Scaled simplex
data: last-modified
execute: 
  cache: true
---

$$
\newcommand{\ent}[2]{\text{\bf ent}(#1\mid #2)}
\newcommand{\KL}[2]{\text{\bf KL}(#1\mid #2)}
$$

Consider a scaled version of the problem where the variables lie in the scaled simplex:
$$
\min_{x\in\tau\Delta}\enspace \ent{p}{q} + \frac{1}{2\lambda}\|Ax-b\|^2,
$$
where $\tau$ is any positive scaling. Observe that
$$
\begin{aligned}
\ent{p}{q} + \delta_{\tau\Delta}(p)
&= \sum_{j=1}^n p_j\log\left(\frac{p_j}{q_j}\right) + \delta_{\tau\Delta}(p)\\
&= \tau\sum_{j=1}^n \left(\frac{p_j}{\tau}\right)\log\left(\frac{p_j}{\tau}\frac{\tau}{q_j}\right) + \delta_{\Delta}\left(\frac{p}{τ}\right)\\
&= \tau\KL{p/\tau}{q/\tau},
\end{aligned}
$$
which we recognize as the $\tau$-perspective transform of KL with a rescaled prior. If we wish that this is a well-formed KL problem, we need to have, in addition to nonnegativity of $p$ and $q$,
$$
\sum_{j=1}^n p_j = \tau \quad\text{and}\quad \sum_{j=1}^n q_j = \tau.
$$
The dual problem corresponding the scaled primal problem is
$$
\min_{y\in\mathbb{R}^m}\enspace \tau\log\sum_{i=1}^m \left(\frac{q_j}{\tau}\right)\exp(A^T y) + \frac{\lambda}{2}\|y\|^2 - \langle b, y \rangle.
$${#eq-dual-problem}

### Objective function

Let's isolate $\tau$ in the objective function to faciliate the computation of the value-function derivative:
$$
\begin{aligned}
\tau\log\sum_{j=1}^n \left(\frac{q_j}{\tau}\right)\exp(z_j)
&= \tau\log\frac1\tau\sum_{j=1}^n q_j\exp(z_j)\\
&= \tau\log\frac1\tau + \tau\log\sum_{j=1}^n q_j\exp(z_j)\\
&= \tau\log\sum_{j=1}^n q_j\exp(z_j) - \tau\log\tau.
\end{aligned}
$${#eq-dual-obj}

Informally, it follows from @eq-dual-problem and @eq-dual-obj that the derivative of the optimal value with respect to the scaling τ is
$$
v'(τ) = \log\sum_{j=1}^n q_j\exp(A^Ty) - \log\tau - 1.
$${#eq-value-derivative}

### Test problem
```{julia}
#|code-fold: true
#|code-summary: Import packages
#|output: false
using KLLS: KLLS, solve!, KLLSModel, scale!, regularize!
using LinearAlgebra
using Plots, Random
```

Generate a small test problem where the solution is on the scaled simplex:
```{julia}
Random.seed!(123)
m, n = 3,5
τ = 2.0
A = randn(m,n)
x = τ*(v=rand(n); v/sum(v))
b = A*x
kl = KLLSModel(A, b)
regularize!(kl, 1e-4)
scale!(kl, τ)
```

Let's solve the problem and verify that the solution looks reasonable.
```{julia}
stats = solve!(kl)
println(stats)
println("sum(x) = $(sum(stats.solution))")
```


### Value function and derivative

The following function computes the value and derivative for a given τ.

```{julia}
function value(τ, kl)

    scale!(kl, τ)
    λ = kl.λ
    s = solve!(kl, logging=0, trace=false)
    y = s.residual/λ
    v = s.dual_obj

    # Linearization at τ̅: v(τ) = v(τ̅) + dv(τ̅)(τ - τ̅) 
    dv = KLLS.obj!(kl.lse, A'y) - log(τ) - 1
    return v, dv 
end;
```

Plot the optimal objective value as a function of the scaling τ, and as a check of our derivative, plot the linearization at τ=1.5.

```{julia}
let
  scales = range(1.0, 2.2, length=50)
  vals = [value(t, kl)[1] for t in scales]
  plot(scales, vals,
       label="Dual objective", xlabel="Scale", ylabel="Objective value", title="Dual objective vs Scale")

  τ = 1.5
  v, dv = value(τ, kl)

  plot!(t -> v + dv*(t - τ), 1.4, 1.6, label="Linearization", linestyle=:dash, color=:red)
end
```

### Maximizing the value function

We can use a root-finding procedure to solve the problem
$$
\max_{\tau} v(τ),
$$
where $v(τ)$ is the value function defined in @eq-value-derivative. The following code snippet finds the optimal scaling τ. Because $v$ is concave in $\tau$ (proof needed), we can simply use a root-finding algorithm to find the point at which $v'(t) = 0$.

```{julia}
import Roots
dv(t) = value(t, kl)[2]
topt = Roots.find_zero(dv, 1.0, verbose=true, rtol=1e-3, atol=1e-3)
println("Optimal scaling t = $topt")
```


Now use the built-in function:
```{julia}
topt, xopt = KLLS.maximize!(kl, rtol=1e-3)
```