var documenterSearchIndex = {"docs":
[{"location":"guide/#User-Guide","page":"User Guide","title":"User Guide","text":"","category":"section"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"This guide provides an overview of DualPerspective.jl and instructions for using the package to solve Kullback-Leibler (KL) regularized least squares problems.","category":"page"},{"location":"guide/#Model-Types","page":"User Guide","title":"Model Types","text":"","category":"section"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"DualPerspective.jl provides several model types for different problem scenarios:","category":"page"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"DPModel: The base model for Kullback-Leibler regularized least squares.\nSSModel: A self-scaling model that adapts regularization parameters.\nOTModel: A model specifically designed for optimal transport problems.\nLPModel: A model for linear programming problems.","category":"page"},{"location":"guide/#Basic-Usage","page":"User Guide","title":"Basic Usage","text":"","category":"section"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"Here's a simple workflow for using DualPerspective.jl:","category":"page"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"Create a model instance\nConfigure model parameters if needed\nSolve the model\nAnalyze the results","category":"page"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"using DualPerspective\n\n# Create a model (this is a simple example)\nmodel = DPModel(A, b, c, λ, q)\n\n# Solve the model\nresult = solve!(model)\n\n# Access the solution\nx_optimal = result.x","category":"page"},{"location":"guide/#Working-with-Results","page":"User Guide","title":"Working with Results","text":"","category":"section"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"The solve! function returns an ExecutionStats object containing:","category":"page"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"status: Solver status (:first_order, :max_iter, etc.)\nx: The optimal solution\nobjective: Final objective value\niter: Number of iterations\ntime: Solution time\nAdditional diagnostic information","category":"page"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"You can visualize results using the histogram function if UnicodePlots is available:","category":"page"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"using UnicodePlots\nhistogram(result)","category":"page"},{"location":"guide/#Advanced-Options","page":"User Guide","title":"Advanced Options","text":"","category":"section"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"The solve! function accepts several optional arguments:","category":"page"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"trace: Boolean to enable iteration output (default: false)\nmax_iter: Maximum number of iterations (default: 100)\natol: Absolute tolerance for convergence (default: depends on model type)\nrtol: Relative tolerance for convergence (default: depends on model type)","category":"page"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"solve!(model, trace=true, max_iter=200, atol=1e-8)","category":"page"},{"location":"guide/","page":"User Guide","title":"User Guide","text":"","category":"page"},{"location":"api/#API-Reference","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"This section provides an overview of the main types and functions in DualPerspective.jl.","category":"page"},{"location":"api/#Models","page":"API Reference","title":"Models","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"DualPerspective.jl provides the following model types:","category":"page"},{"location":"api/","page":"API Reference","title":"API Reference","text":"DPModel: Base model for KL-regularized least squares problems\nSSModel: Self-scaling model variant\nOTModel: Model for optimal transport problems \nLPModel: Model for linear programming problems\nrandDPModel: Function to generate random test instances","category":"page"},{"location":"api/#Solvers","page":"API Reference","title":"Solvers","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"The package offers several solvers:","category":"page"},{"location":"api/","page":"API Reference","title":"API Reference","text":"SSTrunkLS: Trust-region solver with line search\nSequentialSolve: Sequential solver for scaled problems\nLevelSet: Level set method solver\nAdaptiveLevelSet: Adaptive level set solver","category":"page"},{"location":"api/#Core-Functions","page":"API Reference","title":"Core Functions","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"Key functions for working with models:","category":"page"},{"location":"api/","page":"API Reference","title":"API Reference","text":"solve!: Solve a model\nscale!: Apply scaling to a model\nscale: Get the scale of a model\nregularize!: Apply regularization\nhistogram: Visualize solution\nreset!: Reset a model to initial state\nupdate_y0!: Update starting point","category":"page"},{"location":"api/#Auto-generated-Documentation","page":"API Reference","title":"Auto-generated Documentation","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"The following section contains auto-generated documentation for exported symbols:","category":"page"},{"location":"api/#DualPerspective.DPModel","page":"API Reference","title":"DualPerspective.DPModel","text":"DPModel\n\nThe Dual perspective data type holds the data for the KL-regularized linear least-squares model. It extends the AbstractNLPModel interface.\n\nInstantiate the model keyword arguments, e.g.,\n\nmodel = DPModel(A=A, b=b; λ=1e-3, C=I)\n\nor using the convenience constructor that makes the first two arguments A and b required, and infers the other arguments from their sizes, e.g.,\n\nmodel = DPModel(A, b; λ=1e-3, C=I)\n\nKeyword fields\n\nA (AbstractMatrix{T}, required): Constraint matrix defining the linear system.\nb (AbstractVector{T}, required): Target vector in the linear system Ax ≈ b.\nc (AbstractVector{T}, default: ones(n)): Cost vector for the objective function.\nq (AbstractVector{T}, default: fill(1/n, n)): Prior distribution vector for KL divergence term.\nλ (T, default: √eps): Regularization parameter controlling the strength of the KL term.\nscale (T, default: one(eltype(A))): Scaling factor for the problem.\nC (AbstractMatrix{T}, default: I): Positive definite scaling matrix for the linear system.\nname (String, default: \"Dual Perspective Model\"): Optional identifier for the problem instance.\n\nExamples\n\nCreate a simple dual perspective model:\n\nA, b = randn(10, 5), randn(10)\nmodel = DPModel(A=A, b=b)\n\n\n\n\n\n","category":"type"},{"location":"api/#DualPerspective.DPModel-Tuple{}","page":"API Reference","title":"DualPerspective.DPModel","text":"DPModel(; kwargs...)\n\nConvenience constructor for the Dual Perspective Model.\n\nKeyword arguments\n\nA: Constraint matrix defining the linear system.\nb: Target vector in the linear system Ax ≈ b.\n\n\n\n\n\n","category":"method"},{"location":"api/#DualPerspective.LPModel","page":"API Reference","title":"DualPerspective.LPModel","text":"Solve the LP problem:\n\nmin ⟨c, x⟩\nsubject to Ax = b, x ≥ 0\n\nusing the self-scaled solver by solving the regularized problem:\n\nminimize_{x ∈ ℝ₊ⁿ} (1/(2λ)) * ||Ax - b||² + ⟨c, x⟩ + ε * H(x)\n\nwhich is equivalent to:\n\nminimize_{x ∈ ℝ₊ⁿ} (1/(2λε)) * ||Ax - b||² + ⟨c/ε, x⟩ + H(x)\n\nwhere ε is the relaxation constant, λ is the feasibility constant, and H(x) is the entropy function.\n\n\n\n\n\n","category":"type"},{"location":"api/#DualPerspective.LPModel-Tuple{Any, Any, Any}","page":"API Reference","title":"DualPerspective.LPModel","text":"Constructs an LPModel for the LP problem.\n\nArguments\n\nA: Constraint matrix\nb: Right-hand side vector\nc: Cost vector\nε: Relaxation constant\nλ: Feasibility constant\nmaximize: If true, solves max ⟨c, x⟩\nkwargs: Additional keyword arguments\n\nReturns\n\nAn instance of LPModel.\n\n\n\n\n\n","category":"method"},{"location":"api/#DualPerspective.SSModel-Union{Tuple{DPModel{T, M, CT, SB, S} where {M, CT, SB<:AbstractVector{T}, S<:AbstractVector{T}}}, Tuple{T}} where T","page":"API Reference","title":"DualPerspective.SSModel","text":"SSModel(kl::DPModel) -> SSModel\n\nCreate a self-scaled model from a Perspectron model.\n\nArguments\n\nkl::DPModel: The Perspectron model to wrap\n\nDescription\n\nThis model is a container for kl and the augmented problem for the self-scaled model  in the variables (y,t).\n\nThe default starting point is (y0, 1.0), where y0 is the starting point of kl.\n\n\n\n\n\n","category":"method"},{"location":"api/#DualPerspective.histogram","page":"API Reference","title":"DualPerspective.histogram","text":"histogram(s:ExecutionStats; kwargs...)\n\nPlot a histogram of the solution.\n\n\n\n\n\n","category":"function"},{"location":"api/#DualPerspective.randDPModel-Tuple{Any, Any}","page":"API Reference","title":"DualPerspective.randDPModel","text":"randDPModel(m, n; λ=1e-3) -> DPModel\n\nGenerate a random PT model. Arguments:\n\nm: number of rows of the matrix A\nn: number of columns of the matrix A\nλ: regularization parameter (default: 1e-3)\n\n\n\n\n\n","category":"method"},{"location":"api/#DualPerspective.regularize!-Union{Tuple{T}, Tuple{DPModel{T, M, CT, SB, S} where {M, CT, SB<:AbstractVector{T}, S<:AbstractVector{T}}, T}} where T","page":"API Reference","title":"DualPerspective.regularize!","text":"regularize!(kl::DPModel{T}, λ::T) where T\n\nSet the regularization parameter of the Perspectron model.\n\n\n\n\n\n","category":"method"},{"location":"api/#DualPerspective.scale!-Union{Tuple{T}, Tuple{DPModel{T, M, CT, SB, S} where {M, CT, SB<:AbstractVector{T}, S<:AbstractVector{T}}, T}} where T","page":"API Reference","title":"DualPerspective.scale!","text":"scale!(kl::DPModel{T}, scale::T) where T\n\nSet the scaling factor of the Perspectron model.\n\n\n\n\n\n","category":"method"},{"location":"api/#DualPerspective.scale-Tuple{DPModel}","page":"API Reference","title":"DualPerspective.scale","text":"scale(kl::DPModel)\n\nGet the scaling factor of the Perspectron model.\n\n\n\n\n\n","category":"method"},{"location":"api/#DualPerspective.solve!-Tuple{LPModel}","page":"API Reference","title":"DualPerspective.solve!","text":"Solves the LP problem using the self-scaled solver.\n\nArguments\n\nlp: The LPModel instance.\nlogging: Level of logging detail (default: 0).\nmonotone: Enforce monotonicity (default: true).\nmax_time: Maximum solving time in seconds (default: 30.0).\nkwargs: Additional keyword arguments.\n\nReturns\n\nA statistics object containing the solution and status.\n\nNotes\n\nIf the solution is found but the residual norm exceeds 1e-1, the status is set to :infeasible.\n\n\n\n\n\n","category":"method"},{"location":"api/#DualPerspective.solve!-Union{Tuple{T}, Tuple{DPModel{T, M, CT, SB, S} where {M, CT, SB<:AbstractVector{T}, S<:AbstractVector{T}}, SSTrunkLS}} where T","page":"API Reference","title":"DualPerspective.solve!","text":"solve!(ss::SSModel, ::SSTrunkLS; kwargs...)\n\nSolve the self-scaled model using Gauss-Newton, via the TrunkLS algorithm.\n\n\n\n\n\n","category":"method"},{"location":"api/#DualPerspective.solve!-Union{Tuple{T}, Tuple{DPModel{T, M, CT, SB, S} where {M, CT, SB<:AbstractVector{T}, S<:AbstractVector{T}}, SequentialSolve}} where T","page":"API Reference","title":"DualPerspective.solve!","text":"solve!(kl::DPModel, ::SequentialSolve; kwargs...) -> ExecutionStats\n\nSolve the KL-regularized least squares problem by finding the optimal scaling parameter t  that maximizes the dual objective. The optimal t is found by applying root-finding to the derivative of the dual objective with respect to t.\n\nArguments\n\nkl: The KL-regularized least squares model to solve\n::SequentialSolve: The sequential solve algorithm type\n\nKeyword Arguments\n\nt::Real=1.0: Initial guess for the scaling parameter\nrtol::Real=1e-6: Relative tolerance for the root-finding optimization\natol::Real=1e-6: Absolute tolerance for the root-finding optimization  \nxatol::Real=1e-6: Absolute tolerance for convergence in t\nxrtol::Real=1e-6: Relative tolerance for convergence in t\nδ::Real=1e-2: Tolerance factor applied to atol and rtol for the inner optimization\nverbose::Bool=false: Whether to print verbose output from root-finding\n\nReturns\n\nAn ExecutionStats struct containing:\n\nSolution status\nRuntime statistics\nOptimal primal and dual solutions\nResiduals and optimality measures\n\n\n\n\n\n","category":"method"},{"location":"api/#LinearOperators.reset!-Tuple{LPModel}","page":"API Reference","title":"LinearOperators.reset!","text":"Resets the LPModel to its initial state.\n\n\n\n\n\n","category":"method"},{"location":"api/","page":"API Reference","title":"API Reference","text":"","category":"page"},{"location":"dev/#Development","page":"Development","title":"Development","text":"","category":"section"},{"location":"dev/#DualPerspective.LogExp","page":"Development","title":"DualPerspective.LogExp","text":"LogExp{T<:AbstractFloat, V1<:AbstractVector{T}, V2<:AbstractVector{T}}\n\nA structure that implements the log-sum-exp (LSE) function with a prior distribution.\n\nThe log-sum-exp function is defined as:\n\nphi^*(p mid q) = logleft(sum_i=1^n q_i e^p_iright)\n\nwhere q is a prior probability vector. This function is the convex conjugate of the  Kullback-Leibler divergence and plays a key role in dual perspective reformulations of optimization problems.\n\nFields\n\nq::V1: Prior probability vector (must be non-negative)\ng::V2: Buffer for storing the gradient of the log-sum-exp function\n\nUsage\n\nConstruct with a prior vector:\n\nlse = LogExp(q)\n\nOr with a dimension for uniform prior:\n\nlse = LogExp(n)\n\nEvaluate the function at a point p:\n\nvalue = obj!(lse, p)\ngradient = grad(lse)\n\nSee also obj!, grad, hess, kl_divergence.\n\n\n\n\n\n","category":"type"},{"location":"dev/#DualPerspective.LogExp-Tuple{AbstractVector}","page":"Development","title":"DualPerspective.LogExp","text":"LogExp(q)\n\nConstruct a LogExp object that implements the log-sum-exp (LSE) function with a prior q. Evaluate the LSE function at a point p with obj!(lse, p); the corresponding gradient is retrieved with grad(lse). The gradient is stored in an internal buffer g of the LogExp object; do not modify this buffer.\n\nIf no prior is known, instead provide the dimension n:\n\nLogExp(n)\n\nwhich will use the uniform prior q = fill(1/n, n).\n\n\n\n\n\n","category":"method"},{"location":"dev/#DualPerspective.grad-Tuple{DualPerspective.LogExp}","page":"Development","title":"DualPerspective.grad","text":"grad(lse::LogExp)\n\nGet the gradient of the log-sum-exp function at the point p where the lse objective was last evaluated.\n\n\n\n\n\n","category":"method"},{"location":"dev/#DualPerspective.hess-Tuple{DualPerspective.LogExp}","page":"Development","title":"DualPerspective.hess","text":"hess(lse::LogExp)\n\nGet the Hessian of the log-sum-exp function at the point p where the lse objective was last evaluated.\n\n\n\n\n\n","category":"method"},{"location":"dev/#DualPerspective.kl_divergence-Tuple{Any, Any}","page":"Development","title":"DualPerspective.kl_divergence","text":"kl_divergence(x, x̄)\n\nCompute the Kullback-Leibler divergence between vectors x and x̄.\n\nImplementation Details\n\nSkips entries where x_j = 0 \nReturns zero if x and x̄ are identical\nNo checks are performed on the validity of the inputs, i.e., that x is a probability vector or that x̄ is positive.\n\n\n\n\n\n","category":"method"},{"location":"dev/#DualPerspective.myfindmax-Tuple{Any}","page":"Development","title":"DualPerspective.myfindmax","text":"Find the maximum element of p and its index. This is significantly faster than the built-in findmax.\n\n\n\n\n\n","category":"method"},{"location":"dev/#DualPerspective.obj!-Tuple{DualPerspective.LogExp, Any}","page":"Development","title":"DualPerspective.obj!","text":"obj!(lse, p)\n\nEvaluates the value and gradient of the log-sum-exp function\n\nf(p) = log(sum(q_i e^{p_i} for i in 1:n))\n\nwhere q is the prior probability vector. The gradient is stored in an internal buffer g of the LogExp object.\n\nImplementation\n\nThis implementation is adapted from MonteCarloMeasurements.jl to incorporate a prior q. It uses the max-normalization trick for numerical stability against under/overflow:\n\nlogleft(sum_i=1^n q_i e^p_iright) = m + logleft(sum_i=1^n q_i e^p_i - mright)\n\nwhere m = \\max_i p_i.\n\nReference: https://github.com/baggepinnen/MonteCarloMeasurements.jl/blob/4f9b688d298157dc24a5b0a518d971221fbe15dd/src/resampling.jl#L10\n\nSee also grad and hess.\n\n\n\n\n\n","category":"method"},{"location":"dev/#DualPerspective.sum_all_but-Tuple{Any, Any}","page":"Development","title":"DualPerspective.sum_all_but","text":"sum_all_but(w, i)\n\nSum all elements of w except the i-th element. Used by obj!.\n\n\n\n\n\n","category":"method"},{"location":"theory/#Theoretical-Development","page":"Theory","title":"Theoretical Development","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"The DualPerspective package reformulates various problem classes, including nonnegative least-squares, linear programming, optimal transport, and Hausdorff moment recovery, as instances of the regularized relative-entropy problem","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"beginequation\nmin_xinR^n_+  textstyle ipc x + tfrac12lambda Ax - b^2_C^-1 \n+ sum_j=1^n x_j logleft(x_jxbar_jright)\nlabeleqprimal\nendequation","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"where the convention 0log0=0 is used. A key idea in this approach is to further reformulate the nonnegative cone constraint x in mathbbR^n_+ as the conic extension of the probability simplex. That is, with the obvious identity","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"R^n_+ = bigcup_tauge0 tau Delta^n quad Delta^n = left x in mathbbR^n_+ mid textstylesum_j=1^n x_j = 1 right","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"we can exploit this structure to develop a powerful solution technique.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"This reformulation allows us to approach the original problem \\eqref{eq:primal} through a sequence of simpler, compactified problems defined over the probability simplex. Each of these problems corresponds to a specific scale factor tau, and their solutions converge to the solution of the original problem as tau approaches the appropriate value.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Moreover, these compactified problems admit dual reformulations with highly favorable properties: their objectives are globally Lipschitz-smooth and strongly-convex with uniformly bounded Hessian matrices. These properties enable the development of efficient algorithms with strong convergence guarantees, which we will explore in detail in subsequent sections.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"We make the blanket assumption that the reference point xbarinrelintDelta^n. This assumption implies that xbar has full support. Otherwise, any variable x_j with xbar_j=0 can be fixed at zero without affecting the optimal solution x^* of the original problem \\eqref{eq:primal}.","category":"page"},{"location":"theory/#Dual-Perspective-Model","page":"Theory","title":"Dual Perspective Model","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"The DualPerspective.DPModel type extends the AbstractNLPModel interface to encapsulate the data for the regularized relative-entropy problem \\eqref{eq:primal}.","category":"page"},{"location":"theory/#DualPerspective.DPModel-theory","page":"Theory","title":"DualPerspective.DPModel","text":"DPModel\n\nThe Dual perspective data type holds the data for the KL-regularized linear least-squares model. It extends the AbstractNLPModel interface.\n\nInstantiate the model keyword arguments, e.g.,\n\nmodel = DPModel(A=A, b=b; λ=1e-3, C=I)\n\nor using the convenience constructor that makes the first two arguments A and b required, and infers the other arguments from their sizes, e.g.,\n\nmodel = DPModel(A, b; λ=1e-3, C=I)\n\nKeyword fields\n\nA (AbstractMatrix{T}, required): Constraint matrix defining the linear system.\nb (AbstractVector{T}, required): Target vector in the linear system Ax ≈ b.\nc (AbstractVector{T}, default: ones(n)): Cost vector for the objective function.\nq (AbstractVector{T}, default: fill(1/n, n)): Prior distribution vector for KL divergence term.\nλ (T, default: √eps): Regularization parameter controlling the strength of the KL term.\nscale (T, default: one(eltype(A))): Scaling factor for the problem.\nC (AbstractMatrix{T}, default: I): Positive definite scaling matrix for the linear system.\nname (String, default: \"Dual Perspective Model\"): Optional identifier for the problem instance.\n\nExamples\n\nCreate a simple dual perspective model:\n\nA, b = randn(10, 5), randn(10)\nmodel = DPModel(A=A, b=b)\n\n\n\n\n\n","category":"type"},{"location":"theory/","page":"Theory","title":"Theory","text":"Create a model with a specific regularization parameter λ, using the convenience constructor:","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"using DualPerspective, Random\nA, b = randn(10, 5), randn(10)\nmodel = DPModel(A, b; λ=1e-3)","category":"page"},{"location":"theory/#Value-function-and-compactification","page":"Theory","title":"Value function and compactification","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"Define the Kullback-Leibler (KL)divergence as the relative-entropy of two discrete densities x and xbar in the probability simplex Delta^n:","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"kappa(x mid xbar) =\nbegincases\nsum_j=1^n x_j logleft(x_jxbar_jright)  textif  (xxbar)inDelta^ntimesR^n_++\n+infty  textotherwise\nendcases","category":"page"},{"location":"theory/#DualPerspective.kl_divergence-theory","page":"Theory","title":"DualPerspective.kl_divergence","text":"kl_divergence(x, x̄)\n\nCompute the Kullback-Leibler divergence between vectors x and x̄.\n\nImplementation Details\n\nSkips entries where x_j = 0 \nReturns zero if x and x̄ are identical\nNo checks are performed on the validity of the inputs, i.e., that x is a probability vector or that x̄ is positive.\n\n\n\n\n\n","category":"function"},{"location":"theory/","page":"Theory","title":"Theory","text":"We introduce a deceptively simple scaling of the KL divergence,","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"kappa_tau(x mid xbar)=taukappa(xtaumidxbartau)","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"The following identify is immediate:","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"kappa_tau(x mid xbar) = kappa(xmidxbar) quad forall tau0","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Note that kappa_tau(cdotmidxbar) is the perspective transform of kappa(cdotmidxbar). With this notation, we can now define the primal function phi_pR^n_+timesR_+toeR by","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"phi_p(xtau)=ipcx + tfrac12lambda Ax - b^2_C^-1 + kappa_tau(x mid xbar)","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Observe that domphi_p=set(xtau)mid xintauDelta^n tau0 coincides with the set of feasible points of the original problem \\eqref{eq:primal}.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"We can now rephrase the original problem \\eqref{eq:primal} as the minimization of the value function vR_+toeR over the set of scales tauge0, where the value function is defined as the solution of a compactified problem parameterized by tau:","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"beginequation\n  min_tauge0 v(tau)\n  quad textwith quad\n  v(tau)=min_xinR^n phi_p(xtau)\n  labeleqcompactified-primal\nendequation","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"The function phi_p is implemented as pObj!.","category":"page"},{"location":"theory/#DualPerspective.pObj!-theory","page":"Theory","title":"DualPerspective.pObj!","text":"pObj!(kl::DPModel, x)\n\nCompute the primal objective function value of the problem defined by kl at point x.\n\nReturns\n\nThe scalar value of the primal objective, with type matching the model's type parameter T\n\nnote: Note\nEvaluating the least-squares residual term requires solving a system of linear equations involving the covariance matrix C, which is currently computed using the \\ operator, i.e., C \\ r.\n\n\n\n\n\n","category":"function"},{"location":"theory/#Dual-representation-of-the-value-function","page":"Theory","title":"Dual representation of the value function","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"Here we derive a dual reformulation of the value function v in \\eqref{eq:compactified-primal}. Observe that, as a function of the first argument alone, the function kappa_tau(cdotmidxbar) that appears in the compactified primal problem \\eqref{eq:compactified-primal} is the perspective transform of the function kappa(cdotmidxbar). The conjugate of this function is given by","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"(kappa_tau)^*(cdotmidxbar) = taukappa^*(cdotmidxbar)","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"where the log-sum-exp function","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"kappa^*(z mid xbar) = logtextstylesum_j=1^n xbar_j exp(z_j)","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"is the convex conjugate of the KL divergence kappa(cdot mid xbar) (Beck, 2017; Section 4.4.10). Using Fenchel duality, we may express the value function v in dual form:","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"beginequation\n  v(tau) = min_xinR^n phi_p(xtau) = max_yinR^m phi_d(ytau)\n  labeleqcompactified-dual\nendequation","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"where the concave dual function is given by","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"phi_d(ytau) = ipb y - tfraclambda2 ipy Cy - taukappa^*(A^T y - c mid xbartau)","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Note the appearance of the scaled reference point xbartau in the argument of the conjugate function, which follows from the scaling in the definition of kappa_tau(cdotmidxbar). Thus we can rewrite the dual function as","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"phi_d(ytau) = ipb y - tfraclambda2 ipy Cy - taukappa^*(A^T y - c mid xbar) - taulogtau","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Equality holds in \\eqref{eq:compactified-dual} because the primal and dual problems are both strictly feasible.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"The function phi_d is implemented as dObj!.","category":"page"},{"location":"theory/#DualPerspective.dObj!-theory","page":"Theory","title":"DualPerspective.dObj!","text":"dObj!(kl::DPModel{T}, y) -> T where T<:AbstractFloat\n\nCompute the dual objective function value at the vector y:\n\nd(y) = -(b∙y - 0.5λ y∙Cy - τ log∑exp(A'y - c) - τlogτ)\n\nThe scale parameter τ is taken from the scale field of kl.\n\nReturns\n\nThe scalar value of the dual objective, with type matching the model's type parameter T.\n\nwarning: Objective sign\nThis function implements a dual objective based on minimization. \n\n\n\n\n\n","category":"function"},{"location":"theory/#Analytical-properties-of-the-dual-objective","page":"Theory","title":"Analytical properties of the dual objective","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"The dual objective function possesses a number of favorable analytical properties that enable efficient numerical optimization and provide strong theoretical guarantees on algorithmic performance.","category":"page"},{"location":"theory/#Differentiability-of-the-value-function","page":"Theory","title":"Differentiability of the value function","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"The dual representation \\eqref{eq:compactified-dual} allows us to analyze the differentiability properties of the value function v(tau). For each tau  0, the optimal dual solution y(tau) satisfies the first-order optimality condition:","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"nabla_y phi_d(y(tau)tau) = 0","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Because the covariance matrix C is positive definite, the Hessian nabla^2_y phi_d(ytau) is negative definite with eigenvalues bounded away from zero. This strong concavity property, combined with the fact that phi_d is twice continuously differentiable with respect to both y and tau, ensures that the implicit function theorem can be applied.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"By the implicit function theorem, the mapping tau mapsto y(tau) is continuously differentiable, with","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"y(tau) = -nabla^2_y phi_d(y(tau)tau)^-1 nabla_tauy^2 phi_d(y(tau)tau)","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Therefore, the value function v(tau) = phi_d(y(tau)tau) is continuously differentiable, with derivative given by the partial derivative of phi_d with respect to tau:","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"beginaligned\nv(tau) \n= partial_tau phi_d(y(tau)tau) \n= -kappa^*(A^T y(tau) - c mid xbar) + logtau + 1\nendaligned","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Moreover, because y(tau) is continuously differentiable and phi_d is twice continuously differentiable, v(tau) is twice continuously differentiable. This smoothness property is crucial for applying efficient root-finding methods to solve v(tau) = 0, which is the optimality condition for the original problem \\eqref{eq:primal}.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"The value function v(tau) is implemented as the function value!. Note that this function implements the negative of the dual objective because the algorithm used to solve the compactified problem is based on minimization.","category":"page"},{"location":"theory/#DualPerspective.value!-theory","page":"Theory","title":"DualPerspective.value!","text":"value!(kl::DPModel, τ; kwargs...) -> (v, dv)\n\nCompute the dual objective value v and its derivative dv with respect to the scaling parameter τ.\n\nnote: Minimum scaling parameter\nThe scaling parameter τ is clamped to at least eps(T) to avoid numerical issues.\n\n\n\n\n\n","category":"function"},{"location":"theory/#Primal-from-dual-solution-map","page":"Theory","title":"Primal-from-dual solution map","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"The strong-duality property that furnished the dual representation \\eqref{eq:compactified-dual} also furnishes the primal-from-dual solution map. Indeed, for a fixed scale parameter tau, the dual optimal solution y_tau must satisfy the optimality condition","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"beginequation\n    nabla_yphi(ytau)=0\n    quadLongleftrightarrowquad\n    A x(y) + lambda C y = b \nendequation","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"where the primal-from dual solution map ymapsto x(y) from R^m to Delta^n is given by","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"beginequation\n    x(y) = taunablakappa^*(A^T y - c mid xbar) = taufrac xbar odot exp(A^T y - c)ipxbar exp(A^T y - c)\n    labeleqprimal-from-dual\nendequation","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Here, exp(cdot) is interpreted as a vector-valued function, and odot denotes the elementwise product of two vectors.","category":"page"},{"location":"theory/#Lipschitz-smoothness-and-strong-concavity","page":"Theory","title":"Lipschitz-smoothness and strong concavity","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"The notion of Lipschitz-smoothness and strong concavity (or convexity) plays a fundamental role in establishing convergence rates for optimization algorithms. For second-order Newton-type methods, these properties are especially relevant as they directly influence both the theoretical convergence rate and practical performance. When a function is Lipschitz-smooth (bounded second derivatives), first-order methods exhibit well-defined convergence rates, while strong concavity ensures uniqueness of solutions and quadratic growth conditions. However, to achieve the quadratic convergence that makes Newton's method so powerful, we additionally need Lipschitz continuity of the Hessian (bounded third derivatives). The following proposition establishes these essential properties for our dual objective function with a fixed scale parameter tau, providing the theoretical foundation for implementing efficient second-order methods with guaranteed rapid convergence in a neighborhood of the solution.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Let mu_min and mu_max be the smallest and largest eigenvalues of C, respectively.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Proposition. For any fixed tau  0, the dual objective function phi_d(cdottau) is globally Lipschitz-smooth with modulus (tauA^2 + lambdamu_max), strongly concave with modulus lambdamu_min, and the Hessian nabla^2_y phi_d(cdottau) is Lipschitz continuous.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"The proof of this proposition relies on the analyzing the spectrum of the Hessian of the dual objective:","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"nabla^2_y phi_d(ytau) = -lambda C + tau AS(x_y)A^T","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"where","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"S(x) = Diag(x) - xx^T","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"maps vectors in the probability simplex Delta^n to positive semidefinite matrices, and we use the shorthand notation x_y = x(y) to denote the primal-from-dual solution map \\eqref{eq:primal-from-dual}.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Observe that for any xinDelta^n, S(x) has rank n-1 and trace 1 - x_2^2, and at least one component of the vector must be no larger than 1n.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"For the tightest bound on the spectral norm, we can examine the extremal case. When x = (1n)e, where e is the vector of all ones, the matrix S(x)=(1n)I - (1n^2)ee^T has eigenvalue 1n with multiplicity n-1 and eigenvalue 0 with multiplicity 1.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"For the general case, let mu_1 geq ldots geq mu_n be the eigenvalues of S(x). Because the matrix is positive semidefinite with trace 1 - x_2^2 leq 1, the largest eigenvalue of S(x) is at most 1:","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"S(x)_2 leq 1 quad forall xinDelta^n","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"This implies that the spectral norm of the Hessian is bounded by","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"nabla^2_y phi_d(ytau)_2 leq lambdamu_max + tauA_2^2","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"The Lipschitz continuity of the Hessian follows from analyzing how the term S(x_y) changes with y. The mapping y mapsto x_y is the gradient of the log-sum-exp function composed with the affine map ymapsto A^Ty - c. Because the composition of Lipschitz continuous functions preserves Lipschitz continuity, the Hessian nabla^2_y phi_d(ytau) is Lipschitz continuous with respect to y. Strong concavity follows from the positive definiteness of C.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"These properties have important algorithmic implications:","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"The Lipschitz smoothness ensures that gradient-based methods have well-defined convergence rates.\nThe strong concavity guarantees a unique maximizer and provides a quadratic growth condition away from the solution.\nThe Lipschitz continuity of the Hessian ensures that Newton's method achieves locally quadratic convergence.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"When combined, these properties enable the implementation of robust second-order methods with superlinear convergence. For the compactified dual problem, Newton's method converges quadratically in a neighborhood of the optimal solution. Moreover, the uniform boundedness of the Hessian (with respect to tau) provides numerical stability when implementing globalized Newton methods with line search, as the condition number of the Hessian remains well-controlled throughout the iterative process.","category":"page"},{"location":"theory/#Convergence-analysis","page":"Theory","title":"Convergence analysis","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"Discuss convergence rate of the method\nProvide theoretical guarantees on solution quality\nAnalyze influence of problem parameters on convergence","category":"page"},{"location":"theory/#Relationship-to-other-methods","page":"Theory","title":"Relationship to other methods","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"Compare to interior point methods\nDiscuss similarities and differences to entropic regularization approaches\nHighlight unique aspects of DualPerspective","category":"page"},{"location":"theory/#Implementation-details","page":"Theory","title":"Implementation details","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"Overview of key algorithmic components\nDiscuss numerical stability and computational efficiency\nProvide links to API documentation for implementation","category":"page"},{"location":"theory/#Numerical-experiments","page":"Theory","title":"Numerical experiments","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"Showcase performance on example problems\nCompare to other state-of-the-art solvers\nDiscuss scalability and sensitivity to problem parameters","category":"page"},{"location":"examples/#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"This section provides examples demonstrating how to use DualPerspective.jl for solving various problems.","category":"page"},{"location":"examples/#Optimal-Transport","page":"Examples","title":"Optimal Transport","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"Solving an optimal transport problem with entropic regularization:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using DualPerspective, LinearAlgebra, Distances\n\n# Create supports for the distributions\nμsupport = νsupport = range(-2, 2; length=100)\n\n# Compute the cost matrix using squared Euclidean distance\nC = pairwise(SqEuclidean(), μsupport', νsupport'; dims=2)\n\n# Create source and target distributions\nμ = normalize!(exp.(-μsupport .^ 2 ./ 0.5^2), 1)                    # Source distribution\nν = normalize!(νsupport .^ 2 .* exp.(-νsupport .^ 2 ./ 0.5^2), 1)   # Target distribution\n\n# Set the regularization parameter (entropy)\nϵ = 0.01 * median(C)\n\n# Create and solve the optimal transport model\not = DualPerspective.OTModel(μ, ν, C, ϵ)\nsolution = solve!(ot, trace=true)\n\n# Visualize the solution if UnicodePlots is available\nusing UnicodePlots\nhistogram(solution)","category":"page"},{"location":"examples/#Random-DPModel","page":"Examples","title":"Random DPModel","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"Creating and solving a random dual perspective model:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using DualPerspective, LinearAlgebra\n\n# Create a random DPModel\nm, n = 10, 20  # dimensions\nmodel = randDPModel(m, n)\n\n# Solve the model\nresult = solve!(model, trace=true)\n\n# Print summary\nprintln(\"Status: \", result.status)\nprintln(\"Objective value: \", result.objective)\nprintln(\"Iterations: \", result.iter)","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"","category":"page"},{"location":"#DualPerspective.jl","page":"Home","title":"DualPerspective.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Efficient solvers for Kullback-Leibler regularized least-squares problems and variations.","category":"page"},{"location":"#Overview","page":"Home","title":"Overview","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package provides efficient algorithms for solving optimization problems with Kullback-Leibler (KL) regularization, with a focus on linear least-squares formulations","category":"page"},{"location":"","page":"Home","title":"Home","text":"min_p in Delta^n tfrac12lambda Ax - b^2_C^-1 + ipc x + KL(x mid q) tagP","category":"page"},{"location":"","page":"Home","title":"Home","text":"where","category":"page"},{"location":"","page":"Home","title":"Home","text":"KL(x mid q) =\nbegincases\nsum_j=1^n x_j logleft(x_jq_jright)  textif  xinDelta\n+infty  textotherwise\nendcases","category":"page"},{"location":"","page":"Home","title":"Home","text":"is the KL divergence between densities x and q in the probability simplex","category":"page"},{"location":"","page":"Home","title":"Home","text":"Delta^n=xinR^n_+ mid  sum_j=1^n x_j = 1","category":"page"},{"location":"","page":"Home","title":"Home","text":"The problem data is defined by","category":"page"},{"location":"","page":"Home","title":"Home","text":"the linear operator A from R^n to R^m\nthe observation vector binR^m\nthe linear cost vector cinR^n (default: c=e)\nthe strictly positive vector qinrelintDelta^n (default: q=en)\nthe positive-definite linear operator C on R^n (default: C=I)\nthe regularization parameter lambda0 (default: λ=√eps(eltype(A))).","category":"page"},{"location":"","page":"Home","title":"Home","text":"The operators A and C can be an explicit matrices or linear maps that implement forward and adjoint products, i.e., A*x and A'*y with vectors x and y.","category":"page"},{"location":"","page":"Home","title":"Home","text":"A dual-perspective approach provides for stable and efficient solution of this problem, including important problem classes such as","category":"page"},{"location":"","page":"Home","title":"Home","text":"linear programming\noptimal transport\nleast squares (including simplex constraints)\nHaussdorff moment problems","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The package is available on the Julia General Registry, and can be installed via","category":"page"},{"location":"","page":"Home","title":"Home","text":"import Pkg; Pkg.add(\"DualPerspective\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"For Python users, the package is available on PyPI:","category":"page"},{"location":"","page":"Home","title":"Home","text":"pip install DualPerspective","category":"page"},{"location":"#Quick-Example","page":"Home","title":"Quick Example","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Here's how to solve a simple optimal transport problem:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using DualPerspective, LinearAlgebra, Distances\n\nμsupport = νsupport = range(-2, 2; length=100)\nC = pairwise(SqEuclidean(), μsupport', νsupport'; dims=2)           # Cost matrix\nμ = normalize!(exp.(-μsupport .^ 2 ./ 0.5^2), 1)                    # Start distribution\nν = normalize!(νsupport .^ 2 .* exp.(-νsupport .^ 2 ./ 0.5^2), 1)   # Target distribution\n\nϵ = 0.01*median(C)                 # Entropy regularization constant\not = DualPerspective.OTModel(μ, ν, C, ϵ)      # Model initialization\nsolution = solve!(ot, trace=true)   # Solution to the OT problem          ","category":"page"},{"location":"#Contents","page":"Home","title":"Contents","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\n    \"guide.md\",\n    \"theory.md\",\n    \"examples.md\",\n    \"api.md\",\n]\nDepth = 1","category":"page"},{"location":"#Documentation-Development","page":"Home","title":"Documentation Development","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you're contributing to the documentation, you can preview your changes locally with automatic updating:","category":"page"},{"location":"#Setting-Up-Live-Previews","page":"Home","title":"Setting Up Live Previews","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Make sure you're in the docs directory\nActivate the docs environment and add required packages:\njulia> using Pkg\njulia> Pkg.activate(\".\")\njulia> Pkg.add([\"LiveServer\", \"Revise\"])\nRun the preview script:\njulia> include(\"preview.jl\")\nThe script will:\nBuild the documentation \nStart a server and open your browser to http://localhost:8000/\nWatch for changes in both the documentation files and source code docstrings\nAutomatically rebuild the documentation and refresh the browser when changes are detected\nYou can edit:\nDocumentation markdown files in docs/src/\nSource code docstrings in src/\nBoth will trigger automatic rebuilds and browser refresh","category":"page"},{"location":"","page":"Home","title":"Home","text":"This provides a smooth workflow for documentation development, with real-time previews as you make changes. ","category":"page"},{"location":"refs/#References","page":"References","title":"References","text":"","category":"section"},{"location":"refs/","page":"References","title":"References","text":"Beck, A. (2017). First-Order Methods in Optimization (Society for Industrial and Applied Mathematics, Philadelphia, PA). Accessed on May 30, 2024.\n\n\n\n","category":"page"}]
}
