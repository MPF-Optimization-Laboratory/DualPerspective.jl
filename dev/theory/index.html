<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Theory · DualPerspective.jl</title><meta name="title" content="Theory · DualPerspective.jl"/><meta property="og:title" content="Theory · DualPerspective.jl"/><meta property="twitter:title" content="Theory · DualPerspective.jl"/><meta name="description" content="Documentation for DualPerspective.jl."/><meta property="og:description" content="Documentation for DualPerspective.jl."/><meta property="twitter:description" content="Documentation for DualPerspective.jl."/><meta property="og:url" content="https://MPF-Optimization-Laboratory.github.io/DualPerspective.jl/stable/theory/"/><meta property="twitter:url" content="https://MPF-Optimization-Laboratory.github.io/DualPerspective.jl/stable/theory/"/><link rel="canonical" href="https://MPF-Optimization-Laboratory.github.io/DualPerspective.jl/stable/theory/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">DualPerspective.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li class="is-active"><a class="tocitem" href>Theory</a><ul class="internal"><li><a class="tocitem" href="#Dual-Perspective-Model"><span>Dual Perspective Model</span></a></li><li><a class="tocitem" href="#Value-function-and-compactification"><span>Value function and compactification</span></a></li><li><a class="tocitem" href="#Dual-representation-of-the-value-function"><span>Dual representation of the value function</span></a></li><li><a class="tocitem" href="#Analytical-properties-of-the-dual-objective"><span>Analytical properties of the dual objective</span></a></li><li><a class="tocitem" href="#Convergence-analysis"><span>Convergence analysis</span></a></li><li><a class="tocitem" href="#Relationship-to-other-methods"><span>Relationship to other methods</span></a></li><li><a class="tocitem" href="#Implementation-details"><span>Implementation details</span></a></li><li><a class="tocitem" href="#Numerical-experiments"><span>Numerical experiments</span></a></li></ul></li><li><a class="tocitem" href="../density/">Density Estimation</a></li><li><a class="tocitem" href="../examples/">Examples</a></li><li><a class="tocitem" href="../api/">API Reference</a></li><li><a class="tocitem" href="../dev/">Development</a></li><li><a class="tocitem" href="../refs/">References</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Theory</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Theory</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/MPF-Optimization-Laboratory/DualPerspective.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/MPF-Optimization-Laboratory/DualPerspective.jl/blob/main/docs/src/theory.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Theoretical-Development"><a class="docs-heading-anchor" href="#Theoretical-Development">Theoretical Development</a><a id="Theoretical-Development-1"></a><a class="docs-heading-anchor-permalink" href="#Theoretical-Development" title="Permalink"></a></h1><p>The DualPerspective package reformulates various problem classes, including nonnegative least-squares, linear programming, optimal transport, and Hausdorff moment recovery, as instances of the regularized relative-entropy problem</p><p class="math-container">\[\begin{equation}
\min_{x\in\R^n_+}\  \textstyle \ip{c, x} + \tfrac{1}{2\lambda} \|Ax - b\|^2_{C^{-1}} 
+ \sum_{j=1}^n x_j \log\left(x_j/\xbar_j\right),
\label{eq:primal}
\end{equation}\]</p><p>where the convention <span>$0\log0=0$</span> is used. A key idea in this approach is to further reformulate the nonnegative cone constraint <span>$x \in \mathbb{R}^n_+$</span> as the conic extension of the probability simplex. That is, with the obvious identity</p><p class="math-container">\[\R^n_+ = \bigcup_{\tau\ge0} \tau \Delta^n, \quad \Delta^n := \left\{ x \in \mathbb{R}^n_+ \mid \textstyle\sum_{j=1}^n x_j = 1 \right\},\]</p><p>we can exploit this structure to develop a powerful solution technique.</p><p>This reformulation allows us to approach the original problem \eqref{eq:primal} through a sequence of simpler, compactified problems defined over the probability simplex. Each of these problems corresponds to a specific scale factor <span>$\tau$</span>, and their solutions converge to the solution of the original problem as <span>$\tau$</span> approaches the appropriate value.</p><p>Moreover, these compactified problems admit dual reformulations with highly favorable properties: their objectives are globally Lipschitz-smooth and strongly-convex with uniformly bounded Hessian matrices. These properties enable the development of efficient algorithms with strong convergence guarantees, which we will explore in detail in subsequent sections.</p><p>We make the blanket assumption that the reference point <span>$\xbar\in\relint\Delta^n$</span>. This assumption implies that <span>$\xbar$</span> has full support. Otherwise, any variable <span>$x_j$</span> with <span>$\xbar_j=0$</span> can be fixed at zero without affecting the optimal solution <span>$x^*$</span> of the original problem \eqref{eq:primal}.</p><h2 id="Dual-Perspective-Model"><a class="docs-heading-anchor" href="#Dual-Perspective-Model">Dual Perspective Model</a><a id="Dual-Perspective-Model-1"></a><a class="docs-heading-anchor-permalink" href="#Dual-Perspective-Model" title="Permalink"></a></h2><p>The <a href="../api/#DualPerspective.DPModel"><code>DualPerspective.DPModel</code></a> type extends the <code>AbstractNLPModel</code> interface to encapsulate the data for the regularized relative-entropy problem \eqref{eq:primal}.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DualPerspective.DPModel-theory" href="#DualPerspective.DPModel-theory"><code>DualPerspective.DPModel</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">DPModel</code></pre><p>The Dual perspective data type holds the data for the KL-regularized linear least-squares model. It extends the <a href="https://jso.dev/NLPModels.jl/stable/reference/#NLPModels.AbstractNLPModel"><code>AbstractNLPModel</code> interface</a>.</p><p>Instantiate the model keyword arguments, e.g.,</p><pre><code class="language-julia hljs">model = DPModel(A=A, b=b; λ=1e-3, C=I)</code></pre><p>or using the convenience constructor that makes the first two arguments <code>A</code> and <code>b</code> required, and infers the other arguments from their sizes, e.g.,</p><pre><code class="language-julia hljs">model = DPModel(A, b; λ=1e-3, C=I)</code></pre><p><strong>Keyword fields</strong></p><ul><li><code>A</code> (<code>AbstractMatrix{T}</code>, required): Constraint matrix defining the linear system.</li><li><code>b</code> (<code>AbstractVector{T}</code>, required): Target vector in the linear system Ax ≈ b.</li><li><code>c</code> (<code>AbstractVector{T}</code>, default: ones(n)): Cost vector for the objective function.</li><li><code>q</code> (<code>AbstractVector{T}</code>, default: fill(1/n, n)): Prior distribution vector for KL divergence term.</li><li><code>λ</code> (<code>T</code>, default: √eps): Regularization parameter controlling the strength of the KL term.</li><li><code>scale</code> (<code>T</code>, default: one(eltype(A))): Scaling factor for the problem.</li><li><code>C</code> (<code>AbstractMatrix{T}</code>, default: I): Positive definite scaling matrix for the linear system.</li><li><code>name</code> (<code>String</code>, default: &quot;Dual Perspective Model&quot;): Optional identifier for the problem instance.</li></ul><p><strong>Examples</strong></p><p>Create a simple dual perspective model:</p><pre><code class="language-julia hljs">A, b = randn(10, 5), randn(10)
model = DPModel(A=A, b=b)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MPF-Optimization-Laboratory/DualPerspective.jl/blob/7f02e1360427fb772648f87b403b1c33e0523c48/src/dualperspective-model.jl#L1-L32">source</a></section></article><p>Create a model with a specific regularization parameter <code>λ</code>, using the convenience constructor:</p><pre><code class="language-julia hljs">using DualPerspective, Random
A, b = randn(10, 5), randn(10)
model = DPModel(A, b; λ=1e-3)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">┌────────────────────────────────────────────────────────┐
│                 Dual Perspective Model                 │
├────────────────────────────────────────────────────────┤
│ num rows (m)    =         10    ‖b‖₂ =   2.47e+00      │
│ num cols (n)    =          5    λ    =   1.00e-03      │
│ element type    =    Float64    τ    =   1.00e+00      │
│ ‖Ax - b‖₂       =  2.468e+00                           │
│ ‖Ax - b‖₂/‖b‖₂  =  9.973e-01                           │
│ covariance (C)  = uniform  ( 1.000e+00)                │
│ prior (x̄)       = uniform  ( 2.000e-01)                │
│ cost (c)        = constant (-1.000e+00)                │
│ KL(x ∣ x̄)       = computed ( 0.000e+00)                │
└────────────────────────────────────────────────────────┘
</code></pre><h2 id="Value-function-and-compactification"><a class="docs-heading-anchor" href="#Value-function-and-compactification">Value function and compactification</a><a id="Value-function-and-compactification-1"></a><a class="docs-heading-anchor-permalink" href="#Value-function-and-compactification" title="Permalink"></a></h2><p>Define the <a href="../dev/#DualPerspective.kl_divergence-Tuple{Any, Any}"><code>Kullback-Leibler (KL)divergence</code></a> as the relative-entropy of two discrete densities <span>$x$</span> and <span>$\xbar$</span> in the probability simplex <span>$\Delta^n$</span>:</p><p class="math-container">\[\kappa(x \mid \xbar) :=
\begin{cases}
\sum_{j=1}^n x_j \log\left(x_j/\xbar_j\right) &amp; \text{if } (x,\xbar)\in\Delta^n\times\R^n_{++},\\
+\infty &amp; \text{otherwise.}
\end{cases}\]</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DualPerspective.kl_divergence-theory" href="#DualPerspective.kl_divergence-theory"><code>DualPerspective.kl_divergence</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">kl_divergence(x, x̄)</code></pre><p>Compute the Kullback-Leibler divergence between vectors <code>x</code> and <code>x̄</code>.</p><p><strong>Implementation Details</strong></p><ul><li>Skips entries where <code>x_j = 0</code> </li><li>Returns zero if <code>x</code> and <code>x̄</code> are identical</li><li>No checks are performed on the validity of the inputs, i.e., that <code>x</code> is a probability vector or that <code>x̄</code> is positive.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MPF-Optimization-Laboratory/DualPerspective.jl/blob/7f02e1360427fb772648f87b403b1c33e0523c48/src/logexp.jl#L42-L51">source</a></section></article><p>We introduce a deceptively simple scaling of the KL divergence,</p><p class="math-container">\[\kappa_\tau(x \mid \xbar):=\tau\kappa(x/\tau\mid\xbar/\tau).\]</p><p>The following identify is immediate:</p><p class="math-container">\[\kappa_\tau(x \mid \xbar) = \kappa(x\mid\xbar) \quad \forall \tau&gt;0.\]</p><p>Note that <span>$\kappa_\tau(\cdot\mid\xbar)$</span> is the perspective transform of <span>$\kappa(\cdot\mid\xbar)$</span>. With this notation, we can now define the primal function <span>$\phi_p:\R^n_+\times\R_+\to\eR$</span> by</p><p class="math-container">\[\phi_p(x,\tau):=\ip{c,x} + \tfrac{1}{2\lambda} \|Ax - b\|^2_{C^{-1}} + \kappa_\tau(x \mid \xbar).\]</p><p>Observe that <span>$\dom\phi_p=\set{(x,\tau)\mid x\in\tau\Delta^n,\ \tau&gt;0}$</span> coincides with the set of feasible points of the original problem \eqref{eq:primal}.</p><p>We can now rephrase the original problem \eqref{eq:primal} as the minimization of the value function <span>$v:\R_+\to\eR$</span> over the set of scales <span>$\tau\ge0$</span>, where the value function is defined as the solution of a compactified problem parameterized by <span>$\tau$</span>:</p><p class="math-container">\[\begin{equation}
  \min_{\tau\ge0} v(\tau)
  \quad \text{with} \quad
  v(\tau):=\min_{x\in\R^n}\ \phi_p(x,\tau).
  \label{eq:compactified-primal}
\end{equation}\]</p><p>The function <span>$\phi_p$</span> is implemented as <a href="#DualPerspective.pObj!-theory"><code>pObj!</code></a>.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DualPerspective.pObj!-theory" href="#DualPerspective.pObj!-theory"><code>DualPerspective.pObj!</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">pObj!(kl::DPModel, x)</code></pre><p>Compute the primal objective function value of the problem defined by <code>kl</code> at point <code>x</code>.</p><p><strong>Returns</strong></p><ul><li>The scalar value of the primal objective, with type matching the model&#39;s type parameter T</li></ul><div class="admonition is-info" id="Note-6273bbc2e766c1a3"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-6273bbc2e766c1a3" title="Permalink"></a></header><div class="admonition-body"><p>Evaluating the least-squares residual term requires solving a system of linear equations involving the covariance matrix <code>C</code>, which is currently computed using the <code>\</code> operator, i.e., <code>C \ r</code>.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MPF-Optimization-Laboratory/DualPerspective.jl/blob/7f02e1360427fb772648f87b403b1c33e0523c48/src/newtoncg.jl#L116-L127">source</a></section></article><h2 id="Dual-representation-of-the-value-function"><a class="docs-heading-anchor" href="#Dual-representation-of-the-value-function">Dual representation of the value function</a><a id="Dual-representation-of-the-value-function-1"></a><a class="docs-heading-anchor-permalink" href="#Dual-representation-of-the-value-function" title="Permalink"></a></h2><p>Here we derive a dual reformulation of the value function <span>$v$</span> in \eqref{eq:compactified-primal}. Observe that, as a function of the first argument alone, the function <span>$\kappa_\tau(\cdot\mid\xbar)$</span> that appears in the compactified primal problem \eqref{eq:compactified-primal} is the perspective transform of the function <span>$\kappa(\cdot\mid\xbar)$</span>. The conjugate of this function is given by</p><p class="math-container">\[(\kappa_\tau)^*(\cdot\mid\xbar) = \tau\kappa^*(\cdot\mid\xbar),\]</p><p>where the <a href="../dev/#DualPerspective.LogExp-Tuple{AbstractVector}"><code>log-sum-exp</code></a> function</p><p class="math-container">\[\kappa^*(z \mid \xbar) := \log\textstyle\sum_{j=1}^n \xbar_j \exp(z_j)\]</p><p>is the convex conjugate of the KL divergence <span>$\kappa(\cdot \mid \xbar)$</span> (<a href="../refs/#beck_FirstOrderMethodsOptimization_2017">Beck, 2017</a>; Section 4.4.10). Using Fenchel duality, we may express the value function <span>$v$</span> in dual form:</p><p class="math-container">\[\begin{equation}
  v(\tau) = \min_{x\in\R^n} \phi_p(x,\tau) = \max_{y\in\R^m}\ \phi_d(y,\tau),
  \label{eq:compactified-dual}
\end{equation}\]</p><p>where the concave dual function is given by</p><p class="math-container">\[\phi_d(y,\tau) = \ip{b, y} - \tfrac{\lambda}{2} \ip{y, Cy} - \tau\kappa^*(A^T y - c \mid \xbar/\tau).\]</p><p>Note the appearance of the scaled reference point <span>$\xbar/\tau$</span> in the argument of the conjugate function, which follows from the scaling in the definition of <span>$\kappa_\tau(\cdot\mid\xbar)$</span>. Thus we can rewrite the dual function as</p><p class="math-container">\[\phi_d(y,\tau) = \ip{b, y} - \tfrac{\lambda}{2} \ip{y, Cy} - \tau\kappa^*(A^T y - c \mid \xbar) - \tau\log\tau.\]</p><p>Equality holds in \eqref{eq:compactified-dual} because the primal and dual problems are both strictly feasible.</p><p>The function <span>$\phi_d$</span> is implemented as <a href="#DualPerspective.dObj!-theory"><code>dObj!</code></a>.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DualPerspective.dObj!-theory" href="#DualPerspective.dObj!-theory"><code>DualPerspective.dObj!</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">dObj!(kl::DPModel{T}, y) -&gt; T where T&lt;:AbstractFloat</code></pre><p>Compute the dual objective function value at the vector <code>y</code>:</p><pre><code class="nohighlight hljs">d(y) = -(b∙y - 0.5λ y∙Cy - τ log∑exp(A&#39;y - c) - τlogτ)</code></pre><p>The scale parameter <code>τ</code> is taken from the <code>scale</code> field of <code>kl</code>.</p><p><strong>Returns</strong></p><ul><li>The scalar value of the dual objective, with type matching the model&#39;s type parameter <code>T</code>.</li></ul><div class="admonition is-warning" id="Objective-sign-7ddf4764fecddf71"><header class="admonition-header">Objective sign<a class="admonition-anchor" href="#Objective-sign-7ddf4764fecddf71" title="Permalink"></a></header><div class="admonition-body"><p>This function implements a dual objective based on <strong>minimization</strong>. </p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MPF-Optimization-Laboratory/DualPerspective.jl/blob/7f02e1360427fb772648f87b403b1c33e0523c48/src/newtoncg.jl#L32-L47">source</a></section></article><h2 id="Analytical-properties-of-the-dual-objective"><a class="docs-heading-anchor" href="#Analytical-properties-of-the-dual-objective">Analytical properties of the dual objective</a><a id="Analytical-properties-of-the-dual-objective-1"></a><a class="docs-heading-anchor-permalink" href="#Analytical-properties-of-the-dual-objective" title="Permalink"></a></h2><p>The dual objective function possesses a number of favorable analytical properties that enable efficient numerical optimization and provide strong theoretical guarantees on algorithmic performance.</p><h3 id="Differentiability-of-the-value-function"><a class="docs-heading-anchor" href="#Differentiability-of-the-value-function">Differentiability of the value function</a><a id="Differentiability-of-the-value-function-1"></a><a class="docs-heading-anchor-permalink" href="#Differentiability-of-the-value-function" title="Permalink"></a></h3><p>The dual representation \eqref{eq:compactified-dual} allows us to analyze the differentiability properties of the value function <span>$v(\tau)$</span>. For each <span>$\tau &gt; 0$</span>, the optimal dual solution <span>$y(\tau)$</span> satisfies the first-order optimality condition:</p><p class="math-container">\[\nabla_y \phi_d(y(\tau),\tau) = 0.\]</p><p>Because the covariance matrix <span>$C$</span> is positive definite, the Hessian <span>$\nabla^2_y \phi_d(y,\tau)$</span> is negative definite with eigenvalues bounded away from zero. This strong concavity property, combined with the fact that <span>$\phi_d$</span> is twice continuously differentiable with respect to both <span>$y$</span> and <span>$\tau$</span>, ensures that the implicit function theorem can be applied.</p><p>By the implicit function theorem, the mapping <span>$\tau \mapsto y(\tau)$</span> is continuously differentiable, with</p><p class="math-container">\[y&#39;(\tau) = -[\nabla^2_y \phi_d(y(\tau),\tau)]^{-1} \nabla_{\tau,y}^2 \phi_d(y(\tau),\tau).\]</p><p>Therefore, the value function <span>$v(\tau) = \phi_d(y(\tau),\tau)$</span> is continuously differentiable, with derivative given by the partial derivative of <span>$\phi_d$</span> with respect to <span>$\tau$</span>:</p><p class="math-container">\[\begin{aligned}
v&#39;(\tau) 
&amp;= \partial_\tau \phi_d(y(\tau),\tau) \\
&amp;= -\kappa^*(A^T y(\tau) - c \mid \xbar) + \log\tau + 1.
\end{aligned}\]</p><p>Moreover, because <span>$y(\tau)$</span> is continuously differentiable and <span>$\phi_d$</span> is twice continuously differentiable, <span>$v(\tau)$</span> is twice continuously differentiable. This smoothness property is crucial for applying efficient root-finding methods to solve <span>$v&#39;(\tau) = 0$</span>, which is the optimality condition for the original problem \eqref{eq:primal}.</p><p>The value function <span>$v(\tau)$</span> is implemented as the function <a href="#DualPerspective.value!-theory"><code>value!</code></a>. Note that this function implements the <strong>negative</strong> of the dual objective because the algorithm used to solve the compactified problem is based on <strong>minimization</strong>.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DualPerspective.value!-theory" href="#DualPerspective.value!-theory"><code>DualPerspective.value!</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">value!(kl::DPModel, τ; kwargs...) -&gt; (v, dv)</code></pre><p>Compute the dual objective value <code>v</code> and its derivative <code>dv</code> with respect to the scaling parameter <code>τ</code>.</p><div class="admonition is-info" id="Minimum-scaling-parameter-239418c00d073ce7"><header class="admonition-header">Minimum scaling parameter<a class="admonition-anchor" href="#Minimum-scaling-parameter-239418c00d073ce7" title="Permalink"></a></header><div class="admonition-body"><p>The scaling parameter <code>τ</code> is clamped to at least <code>eps(T)</code> to avoid numerical issues.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MPF-Optimization-Laboratory/DualPerspective.jl/blob/7f02e1360427fb772648f87b403b1c33e0523c48/src/sequential-scale.jl#L1-L8">source</a></section></article><h3 id="Primal-from-dual-solution-map"><a class="docs-heading-anchor" href="#Primal-from-dual-solution-map">Primal-from-dual solution map</a><a id="Primal-from-dual-solution-map-1"></a><a class="docs-heading-anchor-permalink" href="#Primal-from-dual-solution-map" title="Permalink"></a></h3><p>The strong-duality property that furnished the dual representation \eqref{eq:compactified-dual} also furnishes the primal-from-dual solution map. Indeed, for a fixed scale parameter <span>$\tau$</span>, the dual optimal solution <span>$y_\tau$</span> must satisfy the optimality condition</p><p class="math-container">\[\begin{equation}
    \nabla_y\phi(y,\tau)=0
    \quad\Longleftrightarrow\quad
    A x(y) + \lambda C y = b \\
\end{equation}\]</p><p>where the primal-from dual solution map <span>$y\mapsto x(y)$</span> from <span>$\R^m$</span> to <span>$\Delta^n$</span> is given by</p><p class="math-container">\[\begin{equation}
    x(y) = \tau\nabla\kappa^*(A^T y - c \mid \xbar) = \tau\frac{ \xbar \odot \exp(A^T y - c)}{\ip{\xbar, \exp(A^T y - c)}}.
    \label{eq:primal-from-dual}
\end{equation}\]</p><p>Here, <span>$\exp(\cdot)$</span> is interpreted as a vector-valued function, and <span>$\odot$</span> denotes the elementwise product of two vectors.</p><h3 id="Lipschitz-smoothness-and-strong-concavity"><a class="docs-heading-anchor" href="#Lipschitz-smoothness-and-strong-concavity">Lipschitz-smoothness and strong concavity</a><a id="Lipschitz-smoothness-and-strong-concavity-1"></a><a class="docs-heading-anchor-permalink" href="#Lipschitz-smoothness-and-strong-concavity" title="Permalink"></a></h3><p>The notion of Lipschitz-smoothness and strong concavity (or convexity) plays a fundamental role in establishing convergence rates for optimization algorithms. For second-order Newton-type methods, these properties are especially relevant as they directly influence both the theoretical convergence rate and practical performance. When a function is Lipschitz-smooth (bounded second derivatives), first-order methods exhibit well-defined convergence rates, while strong concavity ensures uniqueness of solutions and quadratic growth conditions. However, to achieve the quadratic convergence that makes Newton&#39;s method so powerful, we additionally need Lipschitz continuity of the Hessian (bounded third derivatives). The following proposition establishes these essential properties for our dual objective function with a fixed scale parameter <span>$\tau$</span>, providing the theoretical foundation for implementing efficient second-order methods with guaranteed rapid convergence in a neighborhood of the solution.</p><p>Let <span>$\mu_{\min}$</span> and <span>$\mu_{\max}$</span> be the smallest and largest eigenvalues of <span>$C$</span>, respectively.</p><blockquote><p><strong>Proposition.</strong> For any fixed <span>$\tau &gt; 0$</span>, the dual objective function <span>$\phi_d(\cdot,\tau)$</span> is globally Lipschitz-smooth with modulus <span>$(\tau\|A\|^2 + \lambda\mu_{\max})$</span>, strongly concave with modulus <span>$\lambda\mu_{\min}$</span>, and the Hessian <span>$\nabla^2_y \phi_d(\cdot,\tau)$</span> is Lipschitz continuous.</p></blockquote><p>The proof of this proposition relies on the analyzing the spectrum of the Hessian of the dual objective:</p><p class="math-container">\[\nabla^2_y \phi_d(y,\tau) = -\lambda C + \tau AS(x_y)A^T,\]</p><p>where</p><p class="math-container">\[S(x) := \Diag(x) - xx^T\]</p><p>maps vectors in the probability simplex <span>$\Delta^n$</span> to positive semidefinite matrices, and we use the shorthand notation <span>$x_y := x(y)$</span> to denote the primal-from-dual solution map \eqref{eq:primal-from-dual}.</p><p>Observe that for any <span>$x\in\Delta^n$</span>, <span>$S(x)$</span> has rank <span>$n-1$</span> and trace <span>$1 - \|x\|_2^2$</span>, and at least one component of the vector must be no larger than <span>$1/n$</span>.</p><p>For the tightest bound on the spectral norm, we can examine the extremal case. When <span>$x = (1/n)e$</span>, where <span>$e$</span> is the vector of all ones, the matrix <span>$S(x)=(1/n)I - (1/n^2)ee^T$</span> has eigenvalue <span>$1/n$</span> with multiplicity <span>$n-1$</span> and eigenvalue 0 with multiplicity 1.</p><p>For the general case, let <span>$\mu_1 \geq \ldots \geq \mu_n$</span> be the eigenvalues of <span>$S(x)$</span>. Because the matrix is positive semidefinite with trace <span>$1 - \|x\|_2^2 \leq 1$</span>, the largest eigenvalue of <span>$S(x)$</span> is at most <span>$1$</span>:</p><p class="math-container">\[\|S(x)\|_2 \leq 1 \quad \forall x\in\Delta^n.\]</p><p>This implies that the spectral norm of the Hessian is bounded by</p><p class="math-container">\[\|\nabla^2_y \phi_d(y,\tau)\|_2 \leq \lambda\mu_{\max} + \tau\|A\|_2^2\]</p><p>The Lipschitz continuity of the Hessian follows from analyzing how the term <span>$S(x_y)$</span> changes with <span>$y$</span>. The mapping <span>$y \mapsto x_y$</span> is the gradient of the log-sum-exp function composed with the affine map <span>$y\mapsto A^Ty - c$</span>. Because the composition of Lipschitz continuous functions preserves Lipschitz continuity, the Hessian <span>$\nabla^2_y \phi_d(y,\tau)$</span> is Lipschitz continuous with respect to <span>$y$</span>. Strong concavity follows from the positive definiteness of <span>$C$</span>.</p><p>These properties have important algorithmic implications:</p><ol><li>The Lipschitz smoothness ensures that gradient-based methods have well-defined convergence rates.</li><li>The strong concavity guarantees a unique maximizer and provides a quadratic growth condition away from the solution.</li><li>The Lipschitz continuity of the Hessian ensures that Newton&#39;s method achieves locally quadratic convergence.</li></ol><p>When combined, these properties enable the implementation of robust second-order methods with superlinear convergence. For the compactified dual problem, Newton&#39;s method converges quadratically in a neighborhood of the optimal solution. Moreover, the uniform boundedness of the Hessian (with respect to <span>$\tau$</span>) provides numerical stability when implementing globalized Newton methods with line search, as the condition number of the Hessian remains well-controlled throughout the iterative process.</p><h2 id="Convergence-analysis"><a class="docs-heading-anchor" href="#Convergence-analysis">Convergence analysis</a><a id="Convergence-analysis-1"></a><a class="docs-heading-anchor-permalink" href="#Convergence-analysis" title="Permalink"></a></h2><ul><li>Discuss convergence rate of the method</li><li>Provide theoretical guarantees on solution quality</li><li>Analyze influence of problem parameters on convergence</li></ul><h2 id="Relationship-to-other-methods"><a class="docs-heading-anchor" href="#Relationship-to-other-methods">Relationship to other methods</a><a id="Relationship-to-other-methods-1"></a><a class="docs-heading-anchor-permalink" href="#Relationship-to-other-methods" title="Permalink"></a></h2><ul><li>Compare to interior point methods</li><li>Discuss similarities and differences to entropic regularization approaches</li><li>Highlight unique aspects of DualPerspective</li></ul><h2 id="Implementation-details"><a class="docs-heading-anchor" href="#Implementation-details">Implementation details</a><a id="Implementation-details-1"></a><a class="docs-heading-anchor-permalink" href="#Implementation-details" title="Permalink"></a></h2><ul><li>Overview of key algorithmic components</li><li>Discuss numerical stability and computational efficiency</li><li>Provide links to API documentation for implementation</li></ul><h2 id="Numerical-experiments"><a class="docs-heading-anchor" href="#Numerical-experiments">Numerical experiments</a><a id="Numerical-experiments-1"></a><a class="docs-heading-anchor-permalink" href="#Numerical-experiments" title="Permalink"></a></h2><ul><li>Showcase performance on example problems</li><li>Compare to other state-of-the-art solvers</li><li>Discuss scalability and sensitivity to problem parameters</li></ul></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Home</a><a class="docs-footer-nextpage" href="../density/">Density Estimation »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.11.4 on <span class="colophon-date" title="Sunday 18 May 2025 20:06">Sunday 18 May 2025</span>. Using Julia version 1.11.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body><div data-docstringscollapsed="true"></div></html>
