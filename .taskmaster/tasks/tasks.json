{
  "tasks": [
    {
      "id": 1,
      "title": "Implement DPModel Core Structure",
      "description": "Create the main DPModel type that extends AbstractNLPModel from NLPModels.jl with all necessary fields and constructors.",
      "details": "Create the DPModel struct with fields for:\n- Constraint matrix A (supporting both sparse and dense formats)\n- Target vector b\n- Cost vector c\n- Regularization parameter λ\n- Reference point x̄\n- Problem dimensions\n- Internal state for caching\n\nImplement constructors with sensible defaults and type stability. Ensure proper inheritance from AbstractNLPModel to maintain compatibility with the Julia optimization ecosystem. Include validation for inputs (dimensions, positivity constraints).\n\nExample structure:\n```julia\nstruct DPModel <: AbstractNLPModel\n    meta::NLPModelMeta\n    counters::Counters\n    A::Union{Matrix, SparseMatrixCSC}\n    b::Vector\n    c::Vector\n    λ::Real\n    x̄::Vector\n    # Additional fields for caching\n    # ...\n    \n    # Inner constructor with validation\n    function DPModel(A, b, c, λ, x̄; kwargs...)\n        # Dimension validation\n        # Parameter validation\n        # Initialize meta and counters\n        # Return new instance\n    end\nend\n```",
      "testStrategy": "Create unit tests that verify:\n1. Proper construction with various input types\n2. Error handling for invalid inputs (dimension mismatch, negative λ)\n3. Inheritance from AbstractNLPModel\n4. Proper initialization of meta and counters\n5. Compatibility with NLPModels.jl interface functions",
      "priority": "high",
      "dependencies": [],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 2,
      "title": "Implement Basic Utility Functions",
      "description": "Develop essential utility functions for numerical stability, including log-sum-exp operations and Kullback-Leibler divergence calculations.",
      "details": "Implement the following utility functions with numerical stability considerations:\n\n1. Numerically stable log-sum-exp:\n```julia\nfunction logsumexp(x::AbstractVector)\n    u = maximum(x)\n    return u + log(sum(exp.(x .- u)))\nend\n```\n\n2. Kullback-Leibler divergence between probability distributions:\n```julia\nfunction kl_divergence(p::AbstractVector, q::AbstractVector)\n    result = 0.0\n    for (pi, qi) in zip(p, q)\n        if pi > 0\n            result += pi * log(pi / qi)\n        end\n    end\n    return result\nend\n```\n\n3. Scaled KL divergence for the regularization term:\n```julia\nfunction scaled_kl(x::AbstractVector, x̄::AbstractVector)\n    sum(xi > 0 ? xi * log(xi / x̄i) : 0.0 for (xi, x̄i) in zip(x, x̄))\nend\n```\n\n4. Safe logarithm and division operations:\n```julia\nsafe_log(x) = x > 0 ? log(x) : -Inf\nsafe_div(x, y) = y != 0 ? x / y : (x == 0 ? 0.0 : Inf * sign(x))\n```\n\nEnsure all functions handle edge cases appropriately and maintain numerical stability.",
      "testStrategy": "Test utility functions with:\n1. Normal input ranges\n2. Edge cases (zeros, very small values, very large values)\n3. Verify against known analytical results\n4. Test for numerical stability with ill-conditioned inputs\n5. Benchmark performance against naive implementations",
      "priority": "high",
      "dependencies": [],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 3,
      "title": "Implement Primal Objective Function",
      "description": "Develop the primal objective function with KL divergence regularization for the DPModel.",
      "details": "Implement the primal objective function for the regularized relative-entropy problem:\n\nmin_{x∈ℝⁿ₊} ⟨c, x⟩ + 1/(2λ)||Ax - b||²_{C⁻¹} + ∑ⱼ xⱼlog(xⱼ/x̄ⱼ)\n\nThe implementation should:\n1. Calculate the linear cost term ⟨c, x⟩\n2. Calculate the quadratic penalty term 1/(2λ)||Ax - b||²_{C⁻¹}\n3. Calculate the KL divergence regularization term ∑ⱼ xⱼlog(xⱼ/x̄ⱼ)\n4. Combine these terms with appropriate scaling\n\n```julia\nfunction primal_objective(model::DPModel, x::AbstractVector)\n    # Linear cost term\n    cost = dot(model.c, x)\n    \n    # Quadratic penalty term\n    residual = model.A * x - model.b\n    penalty = (1.0 / (2.0 * model.λ)) * dot(residual, residual)\n    \n    # KL divergence regularization\n    reg = scaled_kl(x, model.x̄)\n    \n    return cost + penalty + reg\nend\n```\n\nOverride the NLPModels.obj function to make this compatible with the AbstractNLPModel interface:\n\n```julia\nfunction NLPModels.obj(model::DPModel, x::AbstractVector)\n    return primal_objective(model, x)\nend\n```\n\nEnsure the function handles the case where some components of x are zero (boundary of the nonnegative cone).",
      "testStrategy": "Test the primal objective function with:\n1. Simple test problems with known analytical solutions\n2. Verify each component (cost, penalty, regularization) separately\n3. Test with boundary cases (some x components at zero)\n4. Verify consistency with mathematical formulation\n5. Test for numerical stability with various scales of input data",
      "priority": "high",
      "dependencies": [
        1,
        2
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 4,
      "title": "Implement Dual Objective Function",
      "description": "Develop the dual objective function with log-sum-exp operations for the DPModel.",
      "details": "Implement the dual objective function for the regularized relative-entropy problem. The dual formulation involves log-sum-exp operations and is key to the dual perspective approach.\n\nThe dual objective has the form:\n-bᵀy - λ * log(∑ᵢ x̄ᵢ * exp(-(cᵢ + Aᵢᵀy)/λ))\n\nwhere Aᵢ is the i-th column of A.\n\n```julia\nfunction dual_objective(model::DPModel, y::AbstractVector)\n    # Linear term\n    linear_term = -dot(model.b, y)\n    \n    # Log-sum-exp term with numerical stability\n    scaled_terms = similar(model.c)\n    for i in 1:length(model.c)\n        # Calculate -(cᵢ + Aᵢᵀy)/λ with column extraction optimized for sparse/dense cases\n        col_i = model.A[:, i]\n        scaled_terms[i] = -(model.c[i] + dot(col_i, y)) / model.λ\n    end\n    \n    # Use the numerically stable logsumexp implementation\n    lse_term = model.λ * (logsumexp(scaled_terms) + log(sum(model.x̄)))\n    \n    return linear_term - lse_term\nend\n```\n\nImplement an additional function to compute the gradient of the dual objective:\n\n```julia\nfunction dual_gradient(model::DPModel, y::AbstractVector)\n    # Compute intermediate values for the gradient\n    scaled_terms = similar(model.c)\n    for i in 1:length(model.c)\n        col_i = model.A[:, i]\n        scaled_terms[i] = -(model.c[i] + dot(col_i, y)) / model.λ\n    end\n    \n    # Compute probabilities from dual variables\n    max_term = maximum(scaled_terms)\n    exp_terms = exp.(scaled_terms .- max_term)\n    normalizer = sum(model.x̄ .* exp_terms)\n    probs = model.x̄ .* exp_terms ./ normalizer\n    \n    # Compute gradient: -b + A*probs\n    return -model.b + model.A * probs\nend\n```",
      "testStrategy": "Test the dual objective function with:\n1. Verify dual objective values against known analytical solutions\n2. Check gradient calculations using finite differences\n3. Verify the duality gap on simple problems\n4. Test numerical stability with ill-conditioned problems\n5. Verify that primal solutions recovered from dual variables satisfy KKT conditions",
      "priority": "high",
      "dependencies": [
        1,
        2
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 5,
      "title": "Implement Gradient and Hessian Computations",
      "description": "Develop efficient and numerically stable gradient and Hessian computations for the primal objective function.",
      "details": "Implement gradient and Hessian computations for the primal objective function, ensuring compatibility with the AbstractNLPModel interface. These implementations should be numerically stable and efficient.\n\nGradient implementation:\n```julia\nfunction NLPModels.grad!(model::DPModel, x::AbstractVector, g::AbstractVector)\n    # Linear cost term gradient\n    g .= model.c\n    \n    # Quadratic penalty term gradient\n    residual = model.A * x - model.b\n    penalty_grad = (1.0 / model.λ) * (model.A' * residual)\n    g .+= penalty_grad\n    \n    # KL divergence regularization gradient\n    for i in 1:length(x)\n        if x[i] > 0\n            g[i] += log(x[i] / model.x̄[i]) + 1\n        elseif x[i] == 0\n            g[i] += -Inf  # Subdifferential at boundary\n        end\n    end\n    \n    return g\nend\n```\n\nHessian-vector product implementation (for matrix-free operations):\n```julia\nfunction NLPModels.hprod!(model::DPModel, x::AbstractVector, v::AbstractVector, Hv::AbstractVector; obj_weight=1.0)\n    # Quadratic penalty term Hessian-vector product\n    Av = model.A * v\n    penalty_hv = (obj_weight / model.λ) * (model.A' * Av)\n    \n    # KL divergence regularization Hessian-vector product\n    for i in 1:length(x)\n        if x[i] > 0\n            Hv[i] = penalty_hv[i] + (obj_weight * v[i] / x[i])\n        else\n            Hv[i] = penalty_hv[i]  # Undefined at boundary, use limiting value\n        end\n    end\n    \n    return Hv\nend\n```\n\nFor explicit Hessian construction (when needed):\n```julia\nfunction NLPModels.hess(model::DPModel, x::AbstractVector; obj_weight=1.0)\n    n = length(x)\n    H = (obj_weight / model.λ) * (model.A' * model.A)  # Quadratic penalty term\n    \n    # Add KL divergence regularization Hessian (diagonal)\n    for i in 1:n\n        if x[i] > 0\n            H[i,i] += obj_weight / x[i]\n        end\n    end\n    \n    return H\nend\n```\n\nImplement specialized versions for sparse matrices when appropriate.",
      "testStrategy": "Test gradient and Hessian computations with:\n1. Verify gradients using finite differences\n2. Verify Hessian-vector products against explicit Hessian multiplication\n3. Test with various problem sizes and sparsity patterns\n4. Check numerical stability near boundaries of the feasible region\n5. Verify that Hessian is positive definite for strictly feasible points\n6. Benchmark performance against naive implementations",
      "priority": "high",
      "dependencies": [
        3
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 6,
      "title": "Implement Primal-from-Dual Solution Mapping",
      "description": "Develop functions to recover primal solutions from dual variables using the optimality conditions.",
      "details": "Implement functions to recover primal solutions from dual variables using the KKT optimality conditions. This mapping is a key component of the dual perspective approach.\n\nThe primal solution can be recovered from dual variables y using:\nx_i = x̄_i * exp(-(c_i + A_i^T y)/λ) / Z\nwhere Z is a normalization constant.\n\n```julia\nfunction primal_from_dual(model::DPModel, y::AbstractVector)\n    n = length(model.c)\n    x = similar(model.c)\n    \n    # Compute scaled terms\n    scaled_terms = similar(model.c)\n    for i in 1:n\n        col_i = model.A[:, i]\n        scaled_terms[i] = -(model.c[i] + dot(col_i, y)) / model.λ\n    end\n    \n    # Compute with numerical stability\n    max_term = maximum(scaled_terms)\n    exp_terms = exp.(scaled_terms .- max_term)\n    normalizer = sum(model.x̄ .* exp_terms)\n    \n    # Recover primal variables\n    for i in 1:n\n        x[i] = model.x̄[i] * exp_terms[i] / normalizer\n    end\n    \n    return x\nend\n```\n\nImplement a function to compute the duality gap as a convergence measure:\n\n```julia\nfunction duality_gap(model::DPModel, x::AbstractVector, y::AbstractVector)\n    primal_obj = primal_objective(model, x)\n    dual_obj = dual_objective(model, y)\n    return primal_obj - dual_obj\nend\n```\n\nAlso implement a function to check the KKT conditions:\n\n```julia\nfunction check_kkt(model::DPModel, x::AbstractVector, y::AbstractVector, tol=1e-6)\n    # Primal feasibility: x ≥ 0\n    primal_feas = all(x .>= -tol)\n    \n    # Stationarity: ∇f(x) + A^T y = 0\n    g = similar(x)\n    grad!(model, x, g)\n    stationarity_resid = norm(g + model.A' * y, Inf)\n    \n    # Complementary slackness: y_i * (Ax - b)_i = 0\n    residual = model.A * x - model.b\n    comp_slack_resid = abs(dot(y, residual))\n    \n    return (primal_feas, stationarity_resid < tol, comp_slack_resid < tol)\nend\n```",
      "testStrategy": "Test the primal-from-dual mapping with:\n1. Verify recovered primal solutions satisfy KKT conditions\n2. Test on problems with known analytical solutions\n3. Verify duality gap approaches zero at convergence\n4. Check numerical stability with ill-conditioned problems\n5. Test with various regularization parameters λ\n6. Verify consistency between primal and dual objectives",
      "priority": "medium",
      "dependencies": [
        4
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 7,
      "title": "Implement Newton-CG Solver",
      "description": "Develop a Newton method with conjugate gradient for solving the inner linear systems.",
      "details": "Implement a Newton method with conjugate gradient for solving the inner linear systems. This solver will be used for large-scale problems where explicit Hessian formation is expensive.\n\n```julia\nfunction newton_cg(model::DPModel, x0::AbstractVector; \n                 max_iter=100, \n                 tol=1e-6, \n                 cg_tol=0.1, \n                 cg_max_iter=20,\n                 verbose=false)\n    x = copy(x0)\n    n = length(x)\n    g = similar(x)\n    p = similar(x)\n    \n    # Initialize\n    iter = 0\n    grad!(model, x, g)\n    norm_g0 = norm(g)\n    norm_g = norm_g0\n    \n    while iter < max_iter && norm_g > tol * max(1.0, norm_g0)\n        # Solve Newton system using CG\n        # Hv = λv for implicit Hessian-vector product\n        cg_iter = 0\n        r = copy(g)  # residual\n        p .= 0        # search direction\n        d = copy(r)   # CG direction\n        r_norm2 = dot(r, r)\n        target_r_norm2 = (cg_tol * norm(r))^2\n        \n        while cg_iter < cg_max_iter && r_norm2 > target_r_norm2\n            # Hessian-vector product\n            Hd = similar(d)\n            hprod!(model, x, d, Hd)\n            \n            # CG update\n            alpha = r_norm2 / max(1e-14, dot(d, Hd))\n            p .+= alpha .* d\n            r .-= alpha .* Hd\n            \n            r_norm2_new = dot(r, r)\n            beta = r_norm2_new / r_norm2\n            r_norm2 = r_norm2_new\n            \n            d .= r .+ beta .* d\n            cg_iter += 1\n        end\n        \n        # Negate search direction (we're minimizing)\n        p .= -p\n        \n        # Line search\n        alpha = 1.0\n        fx = obj(model, x)\n        slope = dot(g, p)\n        \n        # Backtracking line search\n        while alpha > 1e-10\n            x_new = x + alpha * p\n            # Project to ensure nonnegativity\n            x_new .= max.(0.0, x_new)\n            fx_new = obj(model, x_new)\n            \n            if fx_new <= fx + 1e-4 * alpha * slope\n                x .= x_new\n                break\n            end\n            alpha *= 0.5\n        end\n        \n        # Update gradient and check convergence\n        grad!(model, x, g)\n        norm_g = norm(g)\n        iter += 1\n        \n        if verbose\n            println(\"Iteration $iter: obj = $(obj(model, x)), |g| = $norm_g\")\n        end\n    end\n    \n    return x, iter, norm_g\nend\n```\n\nImplement a preconditioner for the CG method to improve convergence:\n\n```julia\nfunction diagonal_preconditioner(model::DPModel, x::AbstractVector)\n    n = length(x)\n    P = zeros(n)\n    \n    # Approximate diagonal of Hessian\n    for i in 1:n\n        # Quadratic term contribution\n        P[i] = sum(model.A[j,i]^2 for j in 1:size(model.A, 1)) / model.λ\n        \n        # KL term contribution (diagonal)\n        if x[i] > 0\n            P[i] += 1.0 / x[i]\n        else\n            P[i] += 1e10  # Large value for numerical stability\n        end\n    end\n    \n    # Return function that applies preconditioner\n    return v -> v ./ P\nend\n```",
      "testStrategy": "Test the Newton-CG solver with:\n1. Small test problems with known solutions\n2. Compare against direct Newton method with explicit Hessian\n3. Verify convergence rates (quadratic near solution)\n4. Test with various problem sizes and condition numbers\n5. Benchmark performance and iteration counts\n6. Verify solution quality using KKT conditions\n7. Test the effect of different preconditioners",
      "priority": "high",
      "dependencies": [
        5,
        6
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 8,
      "title": "Implement Newton-LS Solver",
      "description": "Develop a Newton method with linesearch strategies for globalization.",
      "details": "Implement a Newton method with various linesearch strategies for globalization. This solver will provide robust convergence for a wide range of problems.\n\n```julia\nfunction newton_ls(model::DPModel, x0::AbstractVector; \n                 max_iter=100, \n                 tol=1e-6,\n                 linesearch=:armijo,  # Options: :armijo, :wolfe, :backtracking\n                 verbose=false)\n    x = copy(x0)\n    n = length(x)\n    g = similar(x)\n    p = similar(x)\n    H = zeros(n, n)\n    \n    # Initialize\n    iter = 0\n    grad!(model, x, g)\n    norm_g0 = norm(g)\n    norm_g = norm_g0\n    \n    while iter < max_iter && norm_g > tol * max(1.0, norm_g0)\n        # Compute Hessian\n        hess_structure!(model, H)\n        hess_coord!(model, x, H)\n        \n        # Solve Newton system\n        # Add regularization for numerical stability if needed\n        for i in 1:n\n            H[i,i] += 1e-10\n        end\n        \n        # Solve system (with safeguards for non-PD Hessian)\n        try\n            p .= -H \\ g\n        catch\n            # Fallback to gradient descent if Hessian is problematic\n            p .= -g\n            if verbose\n                println(\"Warning: Using gradient descent direction\")\n            end\n        end\n        \n        # Line search based on selected strategy\n        alpha = 1.0\n        fx = obj(model, x)\n        slope = dot(g, p)\n        \n        if linesearch == :armijo\n            # Armijo backtracking\n            c1 = 1e-4  # Sufficient decrease parameter\n            while alpha > 1e-10\n                x_new = x + alpha * p\n                # Project to ensure nonnegativity\n                x_new .= max.(0.0, x_new)\n                fx_new = obj(model, x_new)\n                \n                if fx_new <= fx + c1 * alpha * slope\n                    x .= x_new\n                    break\n                end\n                alpha *= 0.5\n            end\n        elseif linesearch == :wolfe\n            # Wolfe conditions (sufficient decrease and curvature)\n            c1 = 1e-4  # Sufficient decrease parameter\n            c2 = 0.9   # Curvature parameter\n            \n            # Implementation of Wolfe line search\n            # ...\n        elseif linesearch == :backtracking\n            # Simple backtracking with quadratic interpolation\n            # ...\n        end\n        \n        # Update gradient and check convergence\n        grad!(model, x, g)\n        norm_g = norm(g)\n        iter += 1\n        \n        if verbose\n            println(\"Iteration $iter: obj = $(obj(model, x)), |g| = $norm_g, alpha = $alpha\")\n        end\n    end\n    \n    return x, iter, norm_g\nend\n```\n\nImplement helper functions for the different linesearch strategies:\n\n```julia\nfunction armijo_backtracking(model, x, p, fx, g, c1=1e-4, alpha_max=1.0)\n    alpha = alpha_max\n    slope = dot(g, p)\n    \n    while alpha > 1e-10\n        x_new = x + alpha * p\n        # Project to ensure nonnegativity\n        x_new .= max.(0.0, x_new)\n        fx_new = obj(model, x_new)\n        \n        if fx_new <= fx + c1 * alpha * slope\n            return alpha, x_new\n        end\n        alpha *= 0.5\n    end\n    \n    return 0.0, copy(x)  # Failed to find suitable step\nend\n\nfunction wolfe_linesearch(model, x, p, fx, g, c1=1e-4, c2=0.9, alpha_max=1.0)\n    # Implementation of strong Wolfe conditions line search\n    # ...\nend\n```",
      "testStrategy": "Test the Newton-LS solver with:\n1. Compare different linesearch strategies on various test problems\n2. Verify global convergence from different starting points\n3. Test robustness on ill-conditioned problems\n4. Verify convergence rates (quadratic near solution)\n5. Test handling of non-positive definite Hessians\n6. Benchmark performance against Newton-CG\n7. Verify solution quality using KKT conditions",
      "priority": "medium",
      "dependencies": [
        5,
        6
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 9,
      "title": "Implement Gauss-Newton Solver",
      "description": "Develop a specialized Gauss-Newton solver for problems with nonlinear least-squares structure.",
      "details": "Implement a Gauss-Newton solver specialized for problems with nonlinear least-squares structure. This solver approximates the Hessian using the Jacobian of the residual function, which is often more efficient and numerically stable.\n\n```julia\nfunction gauss_newton(model::DPModel, x0::AbstractVector; \n                    max_iter=100, \n                    tol=1e-6,\n                    verbose=false)\n    x = copy(x0)\n    n = length(x)\n    m = length(model.b)  # Number of constraints/residuals\n    \n    # Initialize\n    iter = 0\n    residual = model.A * x - model.b\n    norm_r0 = norm(residual)\n    norm_r = norm_r0\n    \n    while iter < max_iter && norm_r > tol * max(1.0, norm_r0)\n        # Compute Jacobian of residual (A in this case)\n        J = model.A\n        \n        # Compute gradient of objective\n        g = J' * residual / model.λ\n        \n        # Add gradient of regularization term\n        for i in 1:n\n            if x[i] > 0\n                g[i] += model.c[i] + log(x[i] / model.x̄[i]) + 1\n            end\n        end\n        \n        # Gauss-Newton approximation of Hessian: J'J/λ + diag(1/x)\n        H_gn = J' * J / model.λ\n        for i in 1:n\n            if x[i] > 0\n                H_gn[i,i] += 1.0 / x[i]\n            else\n                H_gn[i,i] += 1e10  # Large value for numerical stability\n            end\n        end\n        \n        # Solve Gauss-Newton system\n        p = -H_gn \\ g\n        \n        # Line search\n        alpha = 1.0\n        fx = obj(model, x)\n        slope = dot(g, p)\n        \n        # Backtracking line search\n        while alpha > 1e-10\n            x_new = x + alpha * p\n            # Project to ensure nonnegativity\n            x_new .= max.(0.0, x_new)\n            fx_new = obj(model, x_new)\n            \n            if fx_new <= fx + 1e-4 * alpha * slope\n                x .= x_new\n                break\n            end\n            alpha *= 0.5\n        end\n        \n        # Update residual and check convergence\n        residual = model.A * x - model.b\n        norm_r = norm(residual)\n        iter += 1\n        \n        if verbose\n            println(\"Iteration $iter: obj = $(obj(model, x)), |r| = $norm_r, alpha = $alpha\")\n        end\n    end\n    \n    return x, iter, norm_r\nend\n```\n\nImplement a matrix-free version for large-scale problems:\n\n```julia\nfunction gauss_newton_matrixfree(model::DPModel, x0::AbstractVector; \n                               max_iter=100, \n                               tol=1e-6,\n                               cg_tol=0.1,\n                               cg_max_iter=20,\n                               verbose=false)\n    # Similar to gauss_newton but uses CG to solve the inner linear system\n    # without explicitly forming the Gauss-Newton Hessian approximation\n    # ...\n    \n    # Define a function for Gauss-Newton Hessian-vector product\n    function gn_hessian_product(v)\n        # J'(J*v)/λ + diag(1/x)*v\n        Jv = model.A * v\n        result = model.A' * Jv / model.λ\n        \n        for i in 1:n\n            if x[i] > 0\n                result[i] += v[i] / x[i]\n            else\n                result[i] += 1e10 * v[i]  # Large value for numerical stability\n            end\n        end\n        \n        return result\n    end\n    \n    # Use this function in a CG solver for the Gauss-Newton system\n    # ...\n    \n    return x, iter, norm_r\nend\n```",
      "testStrategy": "Test the Gauss-Newton solver with:\n1. Compare against Newton methods on least-squares problems\n2. Verify convergence rates (should be quadratic for zero-residual problems)\n3. Test with various problem sizes and condition numbers\n4. Benchmark performance and iteration counts\n5. Verify solution quality using KKT conditions\n6. Test the matrix-free implementation against the explicit version\n7. Verify handling of problems with non-zero residuals at the solution",
      "priority": "medium",
      "dependencies": [
        5,
        6
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 10,
      "title": "Implement Sequential Scaling Algorithm",
      "description": "Develop an adaptive scaling algorithm for improved conditioning of the optimization problem.",
      "details": "Implement a sequential scaling algorithm that adaptively scales the problem to improve conditioning. This approach can significantly enhance convergence for ill-conditioned problems.\n\n```julia\nfunction sequential_scaling(model::DPModel, x0::AbstractVector; \n                         max_outer_iter=10,\n                         inner_solver=:newton_cg,\n                         scaling_strategy=:equilibration,\n                         verbose=false,\n                         inner_tol=1e-4,\n                         final_tol=1e-8)\n    x = copy(x0)\n    n = length(x)\n    m = length(model.b)\n    \n    # Initialize scaling factors\n    row_scaling = ones(m)\n    col_scaling = ones(n)\n    \n    for outer_iter in 1:max_outer_iter\n        # Apply scaling to the model\n        scaled_model = scale_model(model, row_scaling, col_scaling)\n        \n        # Solve the scaled problem with the selected inner solver\n        if inner_solver == :newton_cg\n            x_scaled, inner_iter, norm_g = newton_cg(scaled_model, x, tol=inner_tol, verbose=verbose)\n        elseif inner_solver == :newton_ls\n            x_scaled, inner_iter, norm_g = newton_ls(scaled_model, x, tol=inner_tol, verbose=verbose)\n        elseif inner_solver == :gauss_newton\n            x_scaled, inner_iter, norm_r = gauss_newton(scaled_model, x, tol=inner_tol, verbose=verbose)\n        end\n        \n        # Unscale the solution\n        x .= x_scaled ./ col_scaling\n        \n        # Check convergence on the original problem\n        g = similar(x)\n        grad!(model, x, g)\n        if norm(g) <= final_tol\n            if verbose\n                println(\"Converged after $outer_iter outer iterations\")\n            end\n            break\n        end\n        \n        # Update scaling factors based on the selected strategy\n        if scaling_strategy == :equilibration\n            # Equilibration scaling: make rows and columns have similar norms\n            for i in 1:m\n                row_norm = norm(model.A[i,:], 2)\n                row_scaling[i] = 1.0 / max(1e-10, row_norm)\n            end\n            \n            for j in 1:n\n                col_norm = norm(model.A[:,j], 2)\n                col_scaling[j] = 1.0 / max(1e-10, col_norm)\n            end\n        elseif scaling_strategy == :jacobi\n            # Jacobi scaling: based on diagonal elements\n            # ...\n        elseif scaling_strategy == :adaptive\n            # Adaptive scaling based on solution behavior\n            # ...\n        end\n        \n        if verbose\n            println(\"Outer iteration $outer_iter: obj = $(obj(model, x)), |g| = $(norm(g))\")\n        end\n    end\n    \n    return x\nend\n\nfunction scale_model(model::DPModel, row_scaling::AbstractVector, col_scaling::AbstractVector)\n    # Create a new model with scaled data\n    A_scaled = Diagonal(row_scaling) * model.A * Diagonal(1.0 ./ col_scaling)\n    b_scaled = row_scaling .* model.b\n    c_scaled = model.c .* col_scaling\n    x̄_scaled = model.x̄ .* col_scaling\n    \n    return DPModel(A_scaled, b_scaled, c_scaled, model.λ, x̄_scaled)\nend\n```\n\nImplement different scaling strategies:\n\n```julia\nfunction equilibration_scaling(A::AbstractMatrix)\n    m, n = size(A)\n    row_scaling = ones(m)\n    col_scaling = ones(n)\n    \n    # Iterative equilibration\n    for iter in 1:5\n        # Scale rows\n        for i in 1:m\n            row_norm = norm(A[i,:], 2)\n            if row_norm > 0\n                row_scaling[i] = 1.0 / row_norm\n            end\n        end\n        \n        # Scale columns\n        for j in 1:n\n            col_norm = norm(A[:,j], 2)\n            if col_norm > 0\n                col_scaling[j] = 1.0 / col_norm\n            end\n        end\n    end\n    \n    return row_scaling, col_scaling\nend\n```",
      "testStrategy": "Test the sequential scaling algorithm with:\n1. Compare different scaling strategies on ill-conditioned problems\n2. Verify improvement in convergence rates compared to unscaled problems\n3. Test with various problem sizes and condition numbers\n4. Verify solution quality using KKT conditions\n5. Benchmark performance against direct solvers\n6. Test the effect of scaling on different inner solvers\n7. Verify numerical stability for extremely ill-conditioned problems",
      "priority": "medium",
      "dependencies": [
        7,
        8,
        9
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 11,
      "title": "Implement Level-Set Methods for Constrained Problems",
      "description": "Develop level-set methods for solving constrained optimization problems within the dual perspective framework.",
      "details": "Implement level-set methods for solving constrained optimization problems within the dual perspective framework. Level-set methods are particularly useful for problems with constraints on the objective value or other complex constraints.\n\n```julia\nfunction level_set_method(model::DPModel, x0::AbstractVector, target_value::Real; \n                        max_iter=100,\n                        tol=1e-6,\n                        inner_solver=:newton_cg,\n                        verbose=false)\n    x = copy(x0)\n    n = length(x)\n    \n    # Define the level-set function: f(x) - target_value\n    function level_function(model, x)\n        return obj(model, x) - target_value\n    end\n    \n    # Define the gradient of the level-set function\n    function level_gradient!(model, x, g)\n        grad!(model, x, g)\n        return g\n    end\n    \n    # Initialize\n    iter = 0\n    g = similar(x)\n    level_gradient!(model, x, g)\n    level_value = level_function(model, x)\n    norm_g = norm(g)\n    \n    while iter < max_iter && abs(level_value) > tol && norm_g > tol\n        # Compute search direction using projected gradient\n        p = -g\n        \n        # Ensure search direction maintains feasibility\n        for i in 1:n\n            if x[i] <= 0 && p[i] < 0\n                p[i] = 0.0\n            end\n        end\n        \n        # Normalize search direction\n        p_norm = norm(p)\n        if p_norm > 0\n            p ./= p_norm\n        else\n            # No feasible descent direction\n            break\n        end\n        \n        # Line search along the level set\n        alpha = 1.0\n        fx = obj(model, x)\n        \n        # Backtracking line search\n        while alpha > 1e-10\n            x_new = x + alpha * p\n            # Project to ensure nonnegativity\n            x_new .= max.(0.0, x_new)\n            fx_new = obj(model, x_new)\n            level_new = fx_new - target_value\n            \n            # Accept if we're moving closer to the level set\n            if abs(level_new) < abs(level_value) * (1.0 - 1e-4 * alpha)\n                x .= x_new\n                level_value = level_new\n                break\n            end\n            alpha *= 0.5\n        end\n        \n        # Update gradient and check convergence\n        level_gradient!(model, x, g)\n        norm_g = norm(g)\n        iter += 1\n        \n        if verbose\n            println(\"Iteration $iter: obj = $(obj(model, x)), level = $level_value, |g| = $norm_g\")\n        end\n    end\n    \n    return x, iter, level_value\nend\n```\n\nImplement a variant for solving problems with multiple constraints:\n\n```julia\nfunction constrained_level_set(model::DPModel, x0::AbstractVector, constraint_funcs, constraint_grads, constraint_vals; \n                             max_iter=100,\n                             tol=1e-6,\n                             verbose=false)\n    # Implementation for problems with multiple constraints\n    # Uses a merit function approach combining the constraints\n    # ...\n    \n    return x, iter, constraint_violations\nend\n```",
      "testStrategy": "Test the level-set methods with:\n1. Problems with known target objective values\n2. Verify convergence to the level set\n3. Test with various constraint types and target values\n4. Verify solution quality using KKT conditions\n5. Benchmark performance against penalty methods\n6. Test the handling of infeasible target values\n7. Verify numerical stability for problems with multiple constraints",
      "priority": "medium",
      "dependencies": [
        7,
        8
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 12,
      "title": "Implement Specialized Model Types",
      "description": "Develop specialized model types for optimal transport, linear programming, and self-scaling variants.",
      "details": "Implement specialized model types for optimal transport (OTModel), linear programming (LPModel), and self-scaling (SSModel) variants. These specialized models will provide optimized implementations for specific problem classes.\n\n1. Optimal Transport Model:\n```julia\nstruct OTModel <: AbstractNLPModel\n    meta::NLPModelMeta\n    counters::Counters\n    C::Matrix  # Cost matrix\n    a::Vector  # Source distribution\n    b::Vector  # Target distribution\n    λ::Real    # Regularization parameter\n    # Additional fields\n    \n    function OTModel(C::Matrix, a::Vector, b::Vector, λ::Real; kwargs...)\n        # Validate inputs\n        m, n = size(C)\n        @assert length(a) == m \"Source distribution dimension mismatch\"\n        @assert length(b) == n \"Target distribution dimension mismatch\"\n        @assert all(a .>= 0) && isapprox(sum(a), 1.0) \"Source must be a probability distribution\"\n        @assert all(b .>= 0) && isapprox(sum(b), 1.0) \"Target must be a probability distribution\"\n        \n        # Create constraint matrix for the transport problem\n        A = [kron(sparse(I, n, n), ones(1, m)); kron(ones(1, n), sparse(I, m, m))]\n        c = vec(C)\n        \n        # Initialize meta and counters\n        meta = NLPModelMeta(n*m, x0=ones(n*m)/(n*m), name=\"OT Problem\")\n        counters = Counters()\n        \n        return new(meta, counters, C, a, b, λ)\n    end\nend\n\n# Specialized methods for OTModel\nfunction primal_objective(model::OTModel, x::AbstractVector)\n    # Reshape x to matrix form\n    m, n = length(model.a), length(model.b)\n    X = reshape(x, m, n)\n    \n    # Transport cost\n    cost = sum(model.C .* X)\n    \n    # Entropic regularization\n    reg = model.λ * sum(X .* log.(X .+ 1e-16))\n    \n    return cost + reg\nend\n\n# Specialized Sinkhorn algorithm for OT problems\nfunction sinkhorn(model::OTModel, max_iter=1000, tol=1e-8)\n    m, n = length(model.a), length(model.b)\n    K = exp.(-model.C ./ model.λ)\n    \n    # Initialize scaling vectors\n    u = ones(m)\n    v = ones(n)\n    \n    # Sinkhorn iterations\n    for iter in 1:max_iter\n        u = model.a ./ (K * v)\n        v = model.b ./ (K' * u)\n        \n        # Check convergence\n        err = norm(K .* (u * v') * ones(n) - model.a, Inf)\n        if err < tol\n            break\n        end\n    end\n    \n    # Recover transport plan\n    X = Diagonal(u) * K * Diagonal(v)\n    return vec(X)\nend\n```\n\n2. Linear Programming Model:\n```julia\nstruct LPModel <: AbstractNLPModel\n    meta::NLPModelMeta\n    counters::Counters\n    A::Union{Matrix, SparseMatrixCSC}\n    b::Vector\n    c::Vector\n    λ::Real    # Regularization parameter\n    x̄::Vector  # Reference point\n    \n    function LPModel(A, b, c, λ=0.01, x̄=ones(length(c)); kwargs...)\n        # Validate inputs\n        m, n = size(A)\n        @assert length(b) == m \"Constraint dimension mismatch\"\n        @assert length(c) == n \"Cost vector dimension mismatch\"\n        @assert length(x̄) == n \"Reference point dimension mismatch\"\n        \n        # Initialize meta and counters\n        meta = NLPModelMeta(n, x0=x̄, name=\"LP Problem\")\n        counters = Counters()\n        \n        return new(meta, counters, A, b, c, λ, x̄)\n    end\nend\n\n# Specialized methods for LPModel\n# ...\n```\n\n3. Self-Scaling Model:\n```julia\nstruct SSModel <: AbstractNLPModel\n    meta::NLPModelMeta\n    counters::Counters\n    base_model::DPModel\n    scaling_factors::Vector\n    \n    function SSModel(model::DPModel, initial_scaling=ones(model.meta.nvar))\n        # Copy meta and counters from base model\n        meta = deepcopy(model.meta)\n        counters = deepcopy(model.counters)\n        \n        return new(meta, counters, model, initial_scaling)\n    end\nend\n\n# Specialized methods for SSModel that incorporate scaling\nfunction NLPModels.obj(model::SSModel, x::AbstractVector)\n    # Apply scaling and compute objective\n    scaled_x = x .* model.scaling_factors\n    return NLPModels.obj(model.base_model, scaled_x)\nend\n\nfunction NLPModels.grad!(model::SSModel, x::AbstractVector, g::AbstractVector)\n    # Apply scaling to gradient computation\n    scaled_x = x .* model.scaling_factors\n    scaled_g = similar(g)\n    NLPModels.grad!(model.base_model, scaled_x, scaled_g)\n    g .= scaled_g .* model.scaling_factors\n    return g\nend\n\n# Method to update scaling factors\nfunction update_scaling!(model::SSModel, x::AbstractVector)\n    # Compute new scaling based on current solution\n    # ...\n    return model\nend\n```",
      "testStrategy": "Test the specialized model types with:\n1. Verify OTModel with known optimal transport problems\n2. Test Sinkhorn algorithm convergence and accuracy\n3. Compare LPModel against standard LP solvers\n4. Verify SSModel improves conditioning on ill-conditioned problems\n5. Test specialized methods against general DPModel implementations\n6. Verify solution quality using problem-specific metrics\n7. Benchmark performance against general-purpose solvers",
      "priority": "medium",
      "dependencies": [
        1,
        3,
        4
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 13,
      "title": "Implement Numerical Stability Enhancements",
      "description": "Develop robust numerical techniques for log-sum-exp operations and other numerically sensitive computations.",
      "details": "Implement robust numerical techniques for log-sum-exp operations, KL divergence calculations, and other numerically sensitive computations. These enhancements are critical for solving ill-conditioned problems reliably.\n\n1. Enhanced Log-Sum-Exp Implementation:\n```julia\nfunction logsumexp(x::AbstractVector)\n    # Standard approach with maximum shift\n    u = maximum(x)\n    if isinf(u) && u < 0\n        return -Inf  # All inputs are -Inf\n    end\n    return u + log(sum(exp.(x .- u)))\nend\n\n# Specialized version for weighted log-sum-exp\nfunction logsumexp(x::AbstractVector, w::AbstractVector)\n    @assert length(x) == length(w) \"Dimension mismatch\"\n    \n    # Find maximum for numerical stability\n    u = maximum(x)\n    if isinf(u) && u < 0\n        return -Inf  # All inputs are -Inf\n    end\n    \n    # Compute weighted sum with stability\n    s = sum(w[i] * exp(x[i] - u) for i in 1:length(x) if !isinf(x[i]) || x[i] > 0)\n    return u + log(s)\nend\n\n# Log-sum-exp for matrices (useful for optimal transport)\nfunction logsumexp(X::AbstractMatrix; dims=nothing)\n    if dims === nothing\n        return logsumexp(vec(X))\n    else\n        u = maximum(X, dims=dims)\n        return u .+ log.(sum(exp.(X .- u), dims=dims))\n    end\nend\n```\n\n2. Robust KL Divergence Calculation:\n```julia\nfunction kl_divergence(p::AbstractVector, q::AbstractVector)\n    @assert length(p) == length(q) \"Dimension mismatch\"\n    @assert all(p .>= 0) && all(q .>= 0) \"Inputs must be non-negative\"\n    \n    result = 0.0\n    for i in 1:length(p)\n        if p[i] > 0\n            if q[i] > 0\n                result += p[i] * log(p[i] / q[i])\n            else\n                return Inf  # KL divergence is infinite if q[i]=0 and p[i]>0\n            end\n        end\n        # If p[i]=0, the contribution is 0 by convention\n    end\n    return result\nend\n\n# Regularized KL divergence to handle zeros\nfunction regularized_kl(p::AbstractVector, q::AbstractVector, ε=1e-10)\n    p_reg = p .+ ε\n    q_reg = q .+ ε\n    \n    # Normalize to maintain sum=1 property\n    p_reg ./= sum(p_reg)\n    q_reg ./= sum(q_reg)\n    \n    return kl_divergence(p_reg, q_reg)\nend\n```\n\n3. Stable Computation of Gradients and Hessians:\n```julia\nfunction stable_gradient!(model::DPModel, x::AbstractVector, g::AbstractVector)\n    # Linear cost term gradient\n    g .= model.c\n    \n    # Quadratic penalty term gradient\n    residual = model.A * x - model.b\n    penalty_grad = (1.0 / model.λ) * (model.A' * residual)\n    g .+= penalty_grad\n    \n    # KL divergence regularization gradient with stability\n    for i in 1:length(x)\n        if x[i] > 1e-14\n            g[i] += log(max(x[i], 1e-14) / model.x̄[i]) + 1\n        else\n            # Handle boundary case\n            g[i] += -20.0  # Approximation of log(very small) for numerical stability\n        end\n    end\n    \n    return g\nend\n\nfunction stable_hessian_product!(model::DPModel, x::AbstractVector, v::AbstractVector, Hv::AbstractVector)\n    # Quadratic penalty term Hessian-vector product\n    Av = model.A * v\n    penalty_hv = (1.0 / model.λ) * (model.A' * Av)\n    \n    # KL divergence regularization Hessian-vector product with stability\n    for i in 1:length(x)\n        if x[i] > 1e-14\n            Hv[i] = penalty_hv[i] + (v[i] / max(x[i], 1e-14))\n        else\n            # Handle boundary case\n            Hv[i] = penalty_hv[i] + 1e14 * v[i]  # Large but finite value\n        end\n    end\n    \n    return Hv\nend\n```\n\n4. Scaling and Preconditioning:\n```julia\nfunction equilibrate_problem!(model::DPModel)\n    m, n = size(model.A)\n    \n    # Compute row and column norms\n    row_norms = [norm(model.A[i,:]) for i in 1:m]\n    col_norms = [norm(model.A[:,j]) for j in 1:n]\n    \n    # Create scaling matrices\n    D_r = Diagonal(1.0 ./ max.(row_norms, 1e-10))\n    D_c = Diagonal(1.0 ./ max.(col_norms, 1e-10))\n    \n    # Scale the problem data\n    model.A = D_r * model.A * D_c\n    model.b = D_r * model.b\n    model.c = D_c * model.c\n    model.x̄ = D_c * model.x̄\n    \n    return model\nend\n```",
      "testStrategy": "Test the numerical stability enhancements with:\n1. Verify log-sum-exp with extreme values (very large positive/negative)\n2. Test KL divergence with distributions containing zeros\n3. Verify gradient and Hessian computations near the boundary of the feasible region\n4. Test scaling and preconditioning on ill-conditioned problems\n5. Verify solution quality on problems with widely varying scales\n6. Benchmark performance against naive implementations\n7. Test with deliberately challenging numerical examples",
      "priority": "high",
      "dependencies": [
        2,
        3,
        4
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 14,
      "title": "Implement Comprehensive Documentation",
      "description": "Develop thorough documentation including API references, mathematical theory, and usage examples.",
      "details": "Develop comprehensive documentation for the package, including API references, mathematical theory, and usage examples. The documentation should be accessible to both practitioners and researchers.\n\n1. API Documentation Structure:\n```julia\n# Example docstring for DPModel\n\"\"\"\n    DPModel(A, b, c, λ=0.01, x̄=ones(length(c)); kwargs...)\n\nConstruct a Dual Perspective Model for solving regularized optimization problems.\n\n# Arguments\n- `A::Union{Matrix, SparseMatrixCSC}`: Constraint matrix\n- `b::Vector`: Target vector\n- `c::Vector`: Cost vector\n- `λ::Real=0.01`: Regularization parameter\n- `x̄::Vector=ones(length(c))`: Reference point for KL divergence\n\n# Keyword Arguments\n- `name::String=\"DPModel\"`: Name of the model\n- `kwargs...`: Additional arguments passed to NLPModelMeta\n\n# Returns\n- `DPModel`: A model instance that extends AbstractNLPModel\n\n# Examples\n```julia\n# Create a simple DPModel\nA = [1.0 2.0; 3.0 4.0]\nb = [5.0, 6.0]\nc = [1.0, 1.0]\nmodel = DPModel(A, b, c)\n\n# Solve using Newton-CG method\nx, iter, norm_g = newton_cg(model, ones(2))\n```\n\"\"\"\nfunction DPModel(A, b, c, λ=0.01, x̄=ones(length(c)); kwargs...)\n    # Implementation\n    # ...\nend\n```\n\n2. Mathematical Theory Documentation:\n- Create a comprehensive `theory.md` file explaining:\n  - The dual perspective approach\n  - Relationship to interior point methods\n  - Convergence analysis and theoretical guarantees\n  - Derivation of the algorithms\n  - Numerical properties and stability considerations\n\n3. Tutorial Notebooks:\n- Create Jupyter notebooks demonstrating:\n  - Basic usage for different problem types\n  - Parameter selection guidelines\n  - Comparison with other methods\n  - Visualization of results\n  - Performance considerations\n\n4. README Structure:\n```markdown\n# DualPerspective.jl\n\nA Julia package for solving regularized optimization problems using the dual perspective approach.\n\n## Installation\n\n```julia\nusing Pkg\nPkg.add(\"DualPerspective\")\n```\n\n## Features\n\n- Unified framework for various optimization problems\n- Efficient algorithms with strong convergence guarantees\n- Specialized solvers for different problem structures\n- Robust numerical implementation\n\n## Quick Start\n\n```julia\nusing DualPerspective\n\n# Create a model\nA = rand(10, 20)\nb = rand(10)\nc = rand(20)\nmodel = DPModel(A, b, c, 0.01)\n\n# Solve using Newton-CG method\nx, iter, norm_g = newton_cg(model, ones(20))\n\n# Check solution quality\nprintln(\"Objective value: \", obj(model, x))\nprintln(\"Constraint violation: \", norm(A*x - b))\n```\n\n## Documentation\n\nFor more detailed information, see the [full documentation](https://dualperspective.readthedocs.io).\n\n## Examples\n\nSee the `examples/` directory for various applications:\n- Optimal transport\n- Nonnegative least squares\n- Linear programming\n- Moment problems\n\n## Citation\n\nIf you use this package in your research, please cite:\n\n```bibtex\n@article{dualperspective,\n  title={Dual Perspective Approach for Regularized Optimization},\n  author={...},\n  journal={...},\n  year={...}\n}\n```\n```\n\n5. Function Index:\n- Create a comprehensive index of all exported functions with brief descriptions\n- Group functions by category (models, solvers, utilities)\n- Include cross-references to related functions",
      "testStrategy": "Test the documentation with:\n1. Verify all exported functions have proper docstrings\n2. Run all code examples to ensure they work as expected\n3. Check cross-references for consistency\n4. Verify mathematical formulas for correctness\n5. Test documentation rendering in different formats (HTML, PDF)\n6. Have colleagues review for clarity and completeness\n7. Verify compatibility with Julia's built-in help system",
      "priority": "medium",
      "dependencies": [
        1,
        2,
        3,
        4,
        5,
        6,
        7,
        8,
        9,
        10,
        11,
        12,
        13
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 15,
      "title": "Implement Cross-Language Interfaces",
      "description": "Develop interfaces for Python and MATLAB to enable broader accessibility of the package.",
      "details": "Develop interfaces for Python and MATLAB to enable broader accessibility of the package. These interfaces should provide a subset of the full Julia functionality with a focus on ease of use.\n\n1. Python Interface using PyJulia:\n```python\n# dualperspective.py\nimport numpy as np\nfrom julia import Julia\nfrom julia import Main as jl\n\n# Initialize Julia\njl.eval('using DualPerspective')\n\nclass DPModel:\n    \"\"\"Python wrapper for DualPerspective.jl's DPModel\"\"\"\n    \n    def __init__(self, A, b, c, lambda_=0.01, x_bar=None):\n        \"\"\"Initialize a DPModel\n        \n        Parameters\n        ----------\n        A : numpy.ndarray\n            Constraint matrix\n        b : numpy.ndarray\n            Target vector\n        c : numpy.ndarray\n            Cost vector\n        lambda_ : float, optional\n            Regularization parameter, by default 0.01\n        x_bar : numpy.ndarray, optional\n            Reference point, by default ones\n        \"\"\"\n        # Convert numpy arrays to Julia arrays\n        A_jl = jl.eval(f'{A.tolist()}')\n        b_jl = jl.eval(f'{b.tolist()}')\n        c_jl = jl.eval(f'{c.tolist()}')\n        \n        if x_bar is None:\n            self.model = jl.eval(f'DPModel({A_jl}, {b_jl}, {c_jl}, {lambda_})')\n        else:\n            x_bar_jl = jl.eval(f'{x_bar.tolist()}')\n            self.model = jl.eval(f'DPModel({A_jl}, {b_jl}, {c_jl}, {lambda_}, {x_bar_jl})')\n    \n    def solve(self, x0=None, method='newton_cg', max_iter=100, tol=1e-6):\n        \"\"\"Solve the optimization problem\n        \n        Parameters\n        ----------\n        x0 : numpy.ndarray, optional\n            Initial point, by default None\n        method : str, optional\n            Solution method, by default 'newton_cg'\n        max_iter : int, optional\n            Maximum iterations, by default 100\n        tol : float, optional\n            Tolerance, by default 1e-6\n            \n        Returns\n        -------\n        numpy.ndarray\n            Optimal solution\n        int\n            Number of iterations\n        float\n            Final gradient norm\n        \"\"\"\n        if x0 is None:\n            x0 = np.ones(len(self.model.meta.x0))\n        \n        x0_jl = jl.eval(f'{x0.tolist()}')\n        \n        if method == 'newton_cg':\n            result = jl.eval(f'newton_cg({self.model}, {x0_jl}, max_iter={max_iter}, tol={tol})')\n        elif method == 'newton_ls':\n            result = jl.eval(f'newton_ls({self.model}, {x0_jl}, max_iter={max_iter}, tol={tol})')\n        elif method == 'gauss_newton':\n            result = jl.eval(f'gauss_newton({self.model}, {x0_jl}, max_iter={max_iter}, tol={tol})')\n        else:\n            raise ValueError(f\"Unknown method: {method}\")\n        \n        # Convert result back to numpy\n        x = np.array(result[0])\n        iter_count = int(result[1])\n        norm_g = float(result[2])\n        \n        return x, iter_count, norm_g\n    \n    def objective(self, x):\n        \"\"\"Compute the objective value\n        \n        Parameters\n        ----------\n        x : numpy.ndarray\n            Point to evaluate\n            \n        Returns\n        -------\n        float\n            Objective value\n        \"\"\"\n        x_jl = jl.eval(f'{x.tolist()}')\n        return float(jl.eval(f'obj({self.model}, {x_jl})'))\n```\n\n2. MATLAB Interface using MATLAB.jl:\n```matlab\n% DualPerspective.m\nclassdef DualPerspective\n    methods (Static)\n        function model = createModel(A, b, c, lambda, x_bar)\n            % Create a DPModel from MATLAB arrays\n            %\n            % Parameters:\n            %   A: Constraint matrix\n            %   b: Target vector\n            %   c: Cost vector\n            %   lambda: Regularization parameter (default: 0.01)\n            %   x_bar: Reference point (default: ones)\n            \n            if nargin < 4\n                lambda = 0.01;\n            end\n            \n            if nargin < 5\n                x_bar = ones(size(c));\n            end\n            \n            % Call Julia function\n            model = struct();\n            model.A = A;\n            model.b = b;\n            model.c = c;\n            model.lambda = lambda;\n            model.x_bar = x_bar;\n            model.julia_model = true; % Flag for internal use\n            \n            % Initialize Julia if needed\n            if ~exist('juliaEval', 'file')\n                error('MATLAB-Julia interface not initialized. Run initJulia() first.');\n            end\n            \n            % Create model in Julia\n            juliaEval(['using DualPerspective; ' ...\n                       'global matlab_model = DPModel(' ...\n                       'reshape([' num2str(A(:)') '], ' num2str(size(A)) '), ' ...\n                       '[' num2str(b') '], ' ...\n                       '[' num2str(c') '], ' ...\n                       num2str(lambda) ', ' ...\n                       '[' num2str(x_bar') '])']);\n        end\n        \n        function [x, iter, norm_g] = solve(model, x0, method, max_iter, tol)\n            % Solve the optimization problem\n            %\n            % Parameters:\n            %   model: Model created with createModel\n            %   x0: Initial point (default: ones)\n            %   method: Solution method (default: 'newton_cg')\n            %   max_iter: Maximum iterations (default: 100)\n            %   tol: Tolerance (default: 1e-6)\n            \n            if ~isfield(model, 'julia_model') || ~model.julia_model\n                error('Invalid model. Use createModel to create a valid model.');\n            end\n            \n            if nargin < 3 || isempty(x0)\n                x0 = ones(size(model.c));\n            end\n            \n            if nargin < 4 || isempty(method)\n                method = 'newton_cg';\n            end\n            \n            if nargin < 5 || isempty(max_iter)\n                max_iter = 100;\n            end\n            \n            if nargin < 6 || isempty(tol)\n                tol = 1e-6;\n            end\n            \n            % Call Julia solver\n            juliaEval(['result = ' method '(matlab_model, ' ...\n                       '[' num2str(x0') '], ' ...\n                       'max_iter=' num2str(max_iter) ', ' ...\n                       'tol=' num2str(tol) ')']);\n            \n            % Get results\n            x = juliaEvalToMATLAB('result[1]');\n            iter = juliaEvalToMATLAB('result[2]');\n            norm_g = juliaEvalToMATLAB('result[3]');\n        end\n        \n        function val = objective(model, x)\n            % Compute the objective value\n            %\n            % Parameters:\n            %   model: Model created with createModel\n            %   x: Point to evaluate\n            \n            if ~isfield(model, 'julia_model') || ~model.julia_model\n                error('Invalid model. Use createModel to create a valid model.');\n            end\n            \n            % Call Julia function\n            juliaEval(['obj_val = obj(matlab_model, [' num2str(x') '])']);\n            val = juliaEvalToMATLAB('obj_val');\n        end\n        \n        function initJulia()\n            % Initialize Julia interface\n            if ~exist('juliaEval', 'file')\n                % Setup Julia path and initialize\n                % ...\n            end\n            \n            % Load DualPerspective.jl\n            juliaEval('using DualPerspective');\n        end\n    end\nend\n```\n\n3. Julia Helper Functions for Interfaces:\n```julia\n# Interface helpers in DualPerspective.jl\n\n# Helper for Python interface\nfunction solve_from_python(A::Matrix, b::Vector, c::Vector, λ::Real, x̄::Vector, \n                         x0::Vector, method::Symbol, max_iter::Int, tol::Real)\n    model = DPModel(A, b, c, λ, x̄)\n    \n    if method == :newton_cg\n        return newton_cg(model, x0, max_iter=max_iter, tol=tol)\n    elseif method == :newton_ls\n        return newton_ls(model, x0, max_iter=max_iter, tol=tol)\n    elseif method == :gauss_newton\n        return gauss_newton(model, x0, max_iter=max_iter, tol=tol)\n    else\n        error(\"Unknown method: $method\")\n    end\nend\n\n# Helper for MATLAB interface\nfunction solve_from_matlab(A::Matrix, b::Vector, c::Vector, λ::Real, x̄::Vector, \n                         x0::Vector, method::String, max_iter::Int, tol::Real)\n    model = DPModel(A, b, c, λ, x̄)\n    \n    if method == \"newton_cg\"\n        return newton_cg(model, x0, max_iter=max_iter, tol=tol)\n    elseif method == \"newton_ls\"\n        return newton_ls(model, x0, max_iter=max_iter, tol=tol)\n    elseif method == \"gauss_newton\"\n        return gauss_newton(model, x0, max_iter=max_iter, tol=tol)\n    else\n        error(\"Unknown method: $method\")\n    end\nend\n```",
      "testStrategy": "Test the cross-language interfaces with:\n1. Verify Python interface works with various NumPy array types\n2. Test MATLAB interface with different MATLAB matrix formats\n3. Compare results between Julia, Python, and MATLAB implementations\n4. Verify error handling and input validation\n5. Test performance overhead of cross-language calls\n6. Verify documentation and examples in each language\n7. Test installation process in different environments",
      "priority": "low",
      "dependencies": [
        1,
        3,
        4,
        5,
        6,
        7,
        8,
        9
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 16,
      "title": "Implement Comprehensive Logging for Gauss-Newton Method",
      "description": "Enhance the existing Gauss-Newton solver with a comprehensive logging system using Julia's standard logging facilities, including appropriate log levels for different types of information and thorough testing.",
      "details": "Implement a comprehensive logging system for the Gauss-Newton method using Julia's standard logging framework:\n\n1. Add structured logging at appropriate levels:\n   - `@debug` for detailed iteration information:\n     ```julia\n     @debug \"Starting iteration $iter\" x=x f=f_val grad_norm=norm(g)\n     @debug \"Iteration details\" step_size=α decrease=Δf\n     ```\n   - `@info` for high-level progress:\n     ```julia\n     @info \"Starting Gauss-Newton solver\" initial_point=x0 initial_value=f0\n     @info \"Solver converged\" iterations=iter f_final=f_val grad_norm=norm(g) time=elapsed_time\n     ```\n   - `@warn` for convergence issues:\n     ```julia\n     @warn \"Maximum iterations reached without convergence\" iter=max_iter grad_norm=norm(g)\n     ```\n\n2. Ensure zero-cost performance when debug logging is disabled:\n   - Use `@debug` macro which has zero overhead when disabled\n   - Add a check in the function documentation showing how to disable debug logging:\n     ```julia\n     Logging.disable_logging(Logging.Debug)\n     ```\n\n3. Update the function docstring to explain logging behavior:\n   ```julia\n   \"\"\"\n       gauss_newton(model::DPModel, x0::AbstractVector; kwargs...)\n\n   Solve the optimization problem using the Gauss-Newton method.\n\n   # Logging\n   This function uses Julia's standard logging system:\n   - Debug level: Per-iteration details (disabled by default)\n   - Info level: Start and end messages with summary statistics\n   - Warn level: Convergence warnings\n\n   To control logging output:\n   - Enable debug: `Logging.global_logger(Logging.ConsoleLogger(stderr, Logging.Debug))`\n   - Disable all: `Logging.global_logger(Logging.NullLogger())`\n   - Zero-cost debug: `Logging.disable_logging(Logging.Debug)` (recommended for production)\n\n   # Arguments\n   - `model::DPModel`: The optimization model\n   - `x0::AbstractVector`: Initial point\n\n   # Keyword Arguments\n   - `max_iter::Int=100`: Maximum number of iterations\n   - `tol::Float64=1e-6`: Convergence tolerance\n   - ...\n   \"\"\"\n   ```\n\n4. Implement the logging in the main solver loop:\n   ```julia\n   function gauss_newton(model::DPModel, x0::AbstractVector; \n                       max_iter=100, \n                       tol=1e-6,\n                       verbose=false)\n       x = copy(x0)\n       n = length(x)\n       \n       # Initial evaluation\n       f = obj(model, x)\n       g = similar(x)\n       grad!(model, x, g)\n       \n       @info \"Starting Gauss-Newton solver\" initial_point=x0 initial_value=f\n       \n       # Print header for debug output\n       @debug \"Gauss-Newton iterations\" \n       @debug \"Iter      Objective      ||∇f||        Step size     Decrease\"\n       \n       start_time = time()\n       \n       for iter = 1:max_iter\n           @debug \"Starting iteration $iter\" x=x f=f grad_norm=norm(g)\n           \n           # Compute search direction using Gauss-Newton approximation\n           # ...\n           \n           # Line search\n           α = 1.0\n           f_new = obj(model, x + α*d)\n           while f_new > f + 0.1*α*dot(g, d) && α > 1e-10\n               α *= 0.5\n               f_new = obj(model, x + α*d)\n           end\n           \n           # Update\n           x_old, f_old = copy(x), f\n           x .+= α .* d\n           f = f_new\n           grad!(model, x, g)\n           \n           # Log iteration details\n           @debug \"Iteration $iter complete\" objective=f grad_norm=norm(g) step_size=α decrease=(f_old-f)\n           \n           # Check convergence\n           if norm(g) < tol\n               elapsed_time = time() - start_time\n               @info \"Solver converged\" iterations=iter f_final=f grad_norm=norm(g) time=elapsed_time\n               return x, f, iter, true\n           end\n       end\n       \n       # If we get here, we didn't converge\n       elapsed_time = time() - start_time\n       @warn \"Maximum iterations reached without convergence\" iter=max_iter grad_norm=norm(g)\n       @info \"Solver terminated\" iterations=max_iter f_final=f grad_norm=norm(g) time=elapsed_time\n       \n       return x, f, max_iter, false\n   end\n   ```\n\n5. Add examples to the documentation showing different logging configurations:\n   ```julia\n   # Examples\n   \n   # Default usage (Info level only)\n   x, f, iters, converged = gauss_newton(model, x0)\n   \n   # Enable debug output\n   using Logging\n   Logging.global_logger(Logging.ConsoleLogger(stderr, Logging.Debug))\n   x, f, iters, converged = gauss_newton(model, x0)\n   \n   # Silence all output\n   Logging.global_logger(Logging.NullLogger())\n   x, f, iters, converged = gauss_newton(model, x0)\n   \n   # Production setup with zero-cost debug logging\n   Logging.disable_logging(Logging.Debug)\n   x, f, iters, converged = gauss_newton(model, x0)\n   ```",
      "testStrategy": "Implement comprehensive tests to verify the logging functionality:\n\n1. Create a test file `test/test_gauss_newton_logging.jl` with the following test cases:\n\n```julia\n@testset \"Gauss-Newton Logging\" begin\n    # Setup a simple test problem\n    model = create_test_model()\n    x0 = ones(model.meta.nvar)\n    \n    # Test 1: Debug messages appear when debug logging is enabled\n    test_log = IOBuffer()\n    debug_logger = ConsoleLogger(test_log, Logging.Debug)\n    \n    Logging.with_logger(debug_logger) do\n        gauss_newton(model, x0, max_iter=5)\n    end\n    \n    log_output = String(take!(test_log))\n    @test occursin(\"Starting iteration\", log_output)\n    @test occursin(\"Iteration details\", log_output)\n    \n    # Test 2: Debug messages don't appear at Info level\n    test_log = IOBuffer()\n    info_logger = ConsoleLogger(test_log, Logging.Info)\n    \n    Logging.with_logger(info_logger) do\n        gauss_newton(model, x0, max_iter=5)\n    end\n    \n    log_output = String(take!(test_log))\n    @test occursin(\"Starting Gauss-Newton solver\", log_output)\n    @test !occursin(\"Starting iteration\", log_output)\n    \n    # Test 3: Info messages always appear at default level\n    test_log = IOBuffer()\n    default_logger = ConsoleLogger(test_log)\n    \n    Logging.with_logger(default_logger) do\n        gauss_newton(model, x0, max_iter=5)\n    end\n    \n    log_output = String(take!(test_log))\n    @test occursin(\"Starting Gauss-Newton solver\", log_output)\n    @test occursin(\"Solver\", log_output)\n    \n    # Test 4: Warning appears when max iterations reached\n    test_log = IOBuffer()\n    warn_logger = ConsoleLogger(test_log, Logging.Warn)\n    \n    Logging.with_logger(warn_logger) do\n        # Use a problem that won't converge in 2 iterations\n        gauss_newton(model, 100*x0, max_iter=2)\n    end\n    \n    log_output = String(take!(test_log))\n    @test occursin(\"Maximum iterations reached\", log_output)\n    \n    # Test 5: Logging can be completely silenced with NullLogger\n    test_log = IOBuffer()\n    # First log something to the buffer to ensure it's working\n    println(test_log, \"Buffer test\")\n    \n    Logging.with_logger(Logging.NullLogger()) do\n        gauss_newton(model, x0, max_iter=5)\n    end\n    \n    # Reset position to beginning of buffer\n    seekstart(test_log)\n    log_output = String(read(test_log))\n    @test occursin(\"Buffer test\", log_output)\n    @test !occursin(\"Gauss-Newton\", log_output)\n    \n    # Test 6: Performance is not affected when debug logging is disabled\n    Logging.disable_logging(Logging.Debug)\n    \n    # Warm up\n    gauss_newton(model, x0, max_iter=5)\n    \n    # Time with debug disabled\n    t1 = @elapsed gauss_newton(model, x0, max_iter=20)\n    \n    # Enable all logging and time again\n    Logging.disable_logging(Logging.BelowMinLevel)\n    test_log = IOBuffer()\n    debug_logger = ConsoleLogger(test_log, Logging.Debug)\n    \n    t2 = @elapsed Logging.with_logger(debug_logger) do\n        gauss_newton(model, x0, max_iter=20)\n    end\n    \n    # The performance should be similar when debug is disabled\n    # Allow some variation due to system factors\n    @test t1 <= t2 * 1.1  # Debug disabled should be at least as fast\nend\n```\n\n2. Add the test file to the main test suite in `test/runtests.jl`:\n\n```julia\ninclude(\"test_gauss_newton_logging.jl\")\n```\n\n3. Verify the tests pass with:\n\n```bash\njulia --project -e 'using Pkg; Pkg.test()'\n```\n\n4. Manually verify the logging behavior by running examples from the documentation in the REPL and checking the output.\n\n5. Create a benchmark script to verify that there is no performance regression when logging is disabled:\n\n```julia\nusing BenchmarkTools\nusing Logging\n\n# Create a test problem\nmodel = create_large_test_model(1000)  # Larger problem for meaningful benchmarks\nx0 = ones(model.meta.nvar)\n\n# Benchmark with debug logging disabled (production setting)\nLogging.disable_logging(Logging.Debug)\nb1 = @benchmark gauss_newton($model, $x0, max_iter=20)\n\n# Benchmark with all logging disabled\nLogging.global_logger(Logging.NullLogger())\nb2 = @benchmark gauss_newton($model, $x0, max_iter=20)\n\n# Benchmark with debug logging enabled\ndebug_logger = ConsoleLogger(devnull, Logging.Debug)\nLogging.global_logger(debug_logger)\nb3 = @benchmark gauss_newton($model, $x0, max_iter=20)\n\nprintln(\"With debug disabled: \", median(b1))\nprintln(\"With all logging disabled: \", median(b2))\nprintln(\"With debug enabled: \", median(b3))\n\n# The first two should be very close in performance\n```",
      "status": "done",
      "dependencies": [
        9
      ],
      "priority": "high",
      "subtasks": []
    }
  ]
}