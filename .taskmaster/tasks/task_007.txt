# Task ID: 7
# Title: Implement Newton-CG Solver
# Status: pending
# Dependencies: 5, 6
# Priority: high
# Description: Develop a Newton method with conjugate gradient for solving the inner linear systems.
# Details:
Implement a Newton method with conjugate gradient for solving the inner linear systems. This solver will be used for large-scale problems where explicit Hessian formation is expensive.

```julia
function newton_cg(model::DPModel, x0::AbstractVector; 
                 max_iter=100, 
                 tol=1e-6, 
                 cg_tol=0.1, 
                 cg_max_iter=20,
                 verbose=false)
    x = copy(x0)
    n = length(x)
    g = similar(x)
    p = similar(x)
    
    # Initialize
    iter = 0
    grad!(model, x, g)
    norm_g0 = norm(g)
    norm_g = norm_g0
    
    while iter < max_iter && norm_g > tol * max(1.0, norm_g0)
        # Solve Newton system using CG
        # Hv = λv for implicit Hessian-vector product
        cg_iter = 0
        r = copy(g)  # residual
        p .= 0        # search direction
        d = copy(r)   # CG direction
        r_norm2 = dot(r, r)
        target_r_norm2 = (cg_tol * norm(r))^2
        
        while cg_iter < cg_max_iter && r_norm2 > target_r_norm2
            # Hessian-vector product
            Hd = similar(d)
            hprod!(model, x, d, Hd)
            
            # CG update
            alpha = r_norm2 / max(1e-14, dot(d, Hd))
            p .+= alpha .* d
            r .-= alpha .* Hd
            
            r_norm2_new = dot(r, r)
            beta = r_norm2_new / r_norm2
            r_norm2 = r_norm2_new
            
            d .= r .+ beta .* d
            cg_iter += 1
        end
        
        # Negate search direction (we're minimizing)
        p .= -p
        
        # Line search
        alpha = 1.0
        fx = obj(model, x)
        slope = dot(g, p)
        
        # Backtracking line search
        while alpha > 1e-10
            x_new = x + alpha * p
            # Project to ensure nonnegativity
            x_new .= max.(0.0, x_new)
            fx_new = obj(model, x_new)
            
            if fx_new <= fx + 1e-4 * alpha * slope
                x .= x_new
                break
            end
            alpha *= 0.5
        end
        
        # Update gradient and check convergence
        grad!(model, x, g)
        norm_g = norm(g)
        iter += 1
        
        if verbose
            println("Iteration $iter: obj = $(obj(model, x)), |g| = $norm_g")
        end
    end
    
    return x, iter, norm_g
end
```

Implement a preconditioner for the CG method to improve convergence:

```julia
function diagonal_preconditioner(model::DPModel, x::AbstractVector)
    n = length(x)
    P = zeros(n)
    
    # Approximate diagonal of Hessian
    for i in 1:n
        # Quadratic term contribution
        P[i] = sum(model.A[j,i]^2 for j in 1:size(model.A, 1)) / model.λ
        
        # KL term contribution (diagonal)
        if x[i] > 0
            P[i] += 1.0 / x[i]
        else
            P[i] += 1e10  # Large value for numerical stability
        end
    end
    
    # Return function that applies preconditioner
    return v -> v ./ P
end
```

# Test Strategy:
Test the Newton-CG solver with:
1. Small test problems with known solutions
2. Compare against direct Newton method with explicit Hessian
3. Verify convergence rates (quadratic near solution)
4. Test with various problem sizes and condition numbers
5. Benchmark performance and iteration counts
6. Verify solution quality using KKT conditions
7. Test the effect of different preconditioners
