[
  {
    "objectID": "quarto/derivations.html",
    "href": "quarto/derivations.html",
    "title": "Cross Entropy under Linear Constraints",
    "section": "",
    "text": "Define the cross-entropy and log-exponential functions by \\[\n\\begin{aligned}\n\\mathop{\\mathrm{cent}}(p \\mid q) &= \\sum_{i=1}^n p_i \\log(p_i/q_i), \\\\\n\\mathop{\\mathrm{logexp}}(p \\mid q) &= \\log\\sum_{i=1}^n q_i\\exp(p_i),\n\\end{aligned}\n\\] which form a conjugate pair. We consider the pair of dual optimization problems \\[\n\\begin{aligned}\n\\min_{p,\\, y } &\\enspace\\left\\{\\mathop{\\mathrm{cent}}(p \\mid q) + \\lambda/2\\|y\\|^2 \\mid Ax + \\lambda y = b\\right\\}\\\\\n\\min_{y} &\\enspace\\left\\{\\mathop{\\mathrm{logexp}}(A^Ty \\mid q) + \\lambda/2\\|y\\|^2 - \\langle b,y \\rangle\\right\\},\n\\end{aligned}\n\\] where \\(A\\) is an \\(m\\)-by-\\(n\\) matrix, \\(b\\) is an \\(m\\)-vector, and \\(\\lambda\\) is a nonnegative regularization parameter."
  },
  {
    "objectID": "quarto/derivations.html#problem-statement",
    "href": "quarto/derivations.html#problem-statement",
    "title": "Cross Entropy under Linear Constraints",
    "section": "",
    "text": "Define the cross-entropy and log-exponential functions by \\[\n\\begin{aligned}\n\\mathop{\\mathrm{cent}}(p \\mid q) &= \\sum_{i=1}^n p_i \\log(p_i/q_i), \\\\\n\\mathop{\\mathrm{logexp}}(p \\mid q) &= \\log\\sum_{i=1}^n q_i\\exp(p_i),\n\\end{aligned}\n\\] which form a conjugate pair. We consider the pair of dual optimization problems \\[\n\\begin{aligned}\n\\min_{p,\\, y } &\\enspace\\left\\{\\mathop{\\mathrm{cent}}(p \\mid q) + \\lambda/2\\|y\\|^2 \\mid Ax + \\lambda y = b\\right\\}\\\\\n\\min_{y} &\\enspace\\left\\{\\mathop{\\mathrm{logexp}}(A^Ty \\mid q) + \\lambda/2\\|y\\|^2 - \\langle b,y \\rangle\\right\\},\n\\end{aligned}\n\\] where \\(A\\) is an \\(m\\)-by-\\(n\\) matrix, \\(b\\) is an \\(m\\)-vector, and \\(\\lambda\\) is a nonnegative regularization parameter."
  },
  {
    "objectID": "quarto/derivations.html#implicit-approach",
    "href": "quarto/derivations.html#implicit-approach",
    "title": "Cross Entropy under Linear Constraints",
    "section": "Implicit approach",
    "text": "Implicit approach\n\nOptimality conditions\nA pair \\((p, y)\\) is primal-dual optimal if and only if it satisfies the optimality conditions \\[\n\\begin{aligned}\nb &= Ax + \\lambda y, \\\\\nx_j &= \\frac{1}{\\sigma(x)}{q_j\\exp(\\langle a_j,y \\rangle)}, \\quad j\\in[n],\\\\\n\\sigma(x) &= \\sum_{j=1}^n q_j\\exp(\\langle a_j,y \\rangle),\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "quarto/about.html",
    "href": "quarto/about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "python_code/globalNewton.html",
    "href": "python_code/globalNewton.html",
    "title": "MaxEnt",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom PhysicsData import A, b, x0;\n\nEE (50, 400)\nTT (50, 400)\nATTENTION: You picked scalar particle make sure that\nATTENTION: omega integration bounds are set to [0, inf)\nATTENTION: kernel mltiplied by w**2\n(50,)\n\n\n/Users/nick/Desktop/Summer 2024/KL Problem/PhysicsData.py:28: RuntimeWarning: divide by zero encountered in divide\n  GG_rho = g2_rhopipi / (48*np.pi) * mass_rho * ( 1 - 4*mass_pi**2 / omegas**2 )**(3/2) * np.heaviside(omegas - 2*mass_pi , 0.5)\n/Users/nick/Desktop/Summer 2024/KL Problem/PhysicsData.py:28: RuntimeWarning: invalid value encountered in power\n  GG_rho = g2_rhopipi / (48*np.pi) * mass_rho * ( 1 - 4*mass_pi**2 / omegas**2 )**(3/2) * np.heaviside(omegas - 2*mass_pi , 0.5)\n/Users/nick/Desktop/Summer 2024/KL Problem/PhysicsData.py:28: RuntimeWarning: invalid value encountered in multiply\n  GG_rho = g2_rhopipi / (48*np.pi) * mass_rho * ( 1 - 4*mass_pi**2 / omegas**2 )**(3/2) * np.heaviside(omegas - 2*mass_pi , 0.5)\nAuxilliary functions:\ndef KL(x, mu): \n  # assume mu is postive \n  if np.sum(x) &gt; 1.01 or np.sum(x) &lt; 0.99 or np.any(x&lt;0) == True:\n    return np.inf\n  x_ = x[x&gt;0]\n  mu_ = mu[x&gt;0]\n  return np.dot(x_, np.log(x_ / mu_))\n\n\ndef set_x(A, mu, y): \n  a = A.T @ y\n  M = np.max(a) # to prevent overflow\n  x = np.diag(mu) @ np.exp(a - M)\n  x = x / np.sum(x) # x is always in the unit simplex as enforced by the optimality conditions \n  return x\n\n\ndef dual_val(A, mu, b, lmda, y):\n  return  (lmda /2) * np.linalg.norm(y) ** 2 + np.log(np.dot(mu, np.exp(A.T @ y))) - np.dot(b,y) # d(y) in fact the negative of the dual objective viewed from a max POV\n\n\ndef primal_val(A, mu, b, lmda, x):\n  return (1/(2*lmda)) * np.linalg.norm(A @ x - b) ** 2 + KL(x, mu)\ndef LocalNewton(A, mu, b, lmda=10e-12, eps=10e-5, max_iters=100, verbose=True):\n  m, n = A.shape\n  y = np.zeros(m)\n  x = set_x(A, mu, y) # we think of x simply as a function of y useful in computing d_grad and d_hess\n  # but x here happens to converge to the solution of the primal as y converges to a sol of min d(y)\n  \n  gaps = []\n  d_grad = A @ x + lmda * y - b # Jacobian of d(y)\n  k = 0\n\n  while np.linalg.norm(d_grad) &gt; eps and k &lt; max_iters:\n\n    gap = primal_val(A, mu, b, lmda, x) + dual_val(A, mu, b, lmda, y)\n    gaps.append(gap)\n  \n    if k % 1000 == 0 and verbose == True:\n      print('-' * 10)\n      print(f'Iteration: {k+1}')\n      print(f'Primal value: {primal_val(A, mu, b, lmda, x)}')\n      print(f'Dual value: {dual_val(A, mu, b, lmda, y)}')\n      print(f'Iter: {k+1}, Dual Jacobian Norm: {np.linalg.norm(d_grad)}, Primal-Dual Gap: {gap}')\n\n    # Sol to Newton equation\n    d_hess = A @ (np.diag(x) - x @ x.T) @ A.T + lmda * np.identity(m)\n    d = np.linalg.solve(d_hess, -d_grad) # d_hess is postive definite thus will always have a sol\n\n    y = y + d\n\n\n    x = set_x(A, mu, y)\n    d_grad = A @ x + lmda * y - b\n    k += 1\n\n  #print(f\"total iters: {k}\")\n\n  return x, y, gaps\ndef GlobalNewton(A, mu, b, lmda=10e-12, eps=10e-5, max_iters=100, verbose=True, rho=1e-11, p=2.1, beta=0.1, s= 0.25, truth=x0):\n  m, n = A.shape\n  y = np.zeros(m)\n  x = set_x(A, mu, y) # we think of x simply as a function of y useful in computing d_grad and d_hess\n  # but x here happens to converge to the solution of the primal as y converges to a sol of min d(y)\n  \n  gaps = []\n  armijo_steps = []\n  d_grad = A @ x + lmda * y - b # Jacobian of d(y)\n  k = 0\n\n  Gnorm = np.linalg.norm(d_grad)\n  Gnorms = []\n\n  while Gnorm &gt; eps and k &lt; max_iters:\n\n    gap = primal_val(A, mu, b, lmda, x) + dual_val(A, mu, b, lmda, y)\n    gaps.append(gap)\n  \n    if k % 1000 == 0 and verbose == True:\n      print('-' * 10)\n      print(f'Iteration: {k+1}')\n      print(f'Primal value: {primal_val(A, mu, b, lmda, x)}')\n      print(f'Dual value: {dual_val(A, mu, b, lmda, y)}')\n      print(f'Iter: {k+1}, Dual Jacobian Norm: {np.linalg.norm(d_grad)}, Primal-Dual Gap: {gap}')\n\n    # Sol to Newton equation\n    d_hess = A @ (np.diag(x) - x @ x.T) @ A.T + lmda * np.identity(m)\n    d = np.linalg.solve(d_hess, -d_grad) # d_hess is postive definite thus will always have a sol\n\n    if np.dot(d_grad, d) &gt; -rho * np.linalg.norm(d) ** p: # check if we have sufficient decrease wrt the norm of d\n      armijo_steps.append(k)\n      d = -d_grad\n      # Take a gradient step with Armijo rule\n      t = 1\n      j = 0\n      while dual_val(A, mu, b, lmda, y + t*d) &gt; dual_val(A, mu ,b, lmda, y) + (t*s) * np.dot(d_grad, d):\n        t *= beta\n        j += 1\n      y = y + t*d\n      if k % 1000 == 0 and verbose == True:\n        print(f'Insuff decrease: Armijo checked {j} times')\n        print(f'dk norm {np.linalg.norm(d)}')\n        print(f'dot prod {np.dot(d_grad, d)}')\n    else:\n      # Take a \"Newton\" step\n      y = y + d\n\n\n    x = set_x(A, mu, y)\n    d_grad = A @ x + lmda * y - b\n    k += 1\n    Gnorm = np.linalg.norm(d_grad)\n    Gnorms.append(Gnorm)\n\n  print('-' * 10)\n  print(\"Summary:\")\n  print(f\"Total iters: {k}\")\n  print(f\"Armijo steps: {len(armijo_steps)}\")\n  print(f\"Jacobian Norm: {Gnorms[-1]}\")\n  print(f'Primal-Dual Gap: {gap}')\n  print(f'Primal value: {primal_val(A, mu, b, lmda, x)}')\n  print(f'Dual value: {dual_val(A, mu, b, lmda, y)}')\n  print(f'RMS(x-x0): {np.linalg.norm(truth - x) / np.sqrt(n)}')\n  print(f'RMS(Ax-b): {np.linalg.norm(A @ x - b) / np.sqrt(m)}')\n\n  return x, y, gaps, armijo_steps, Gnorms"
  },
  {
    "objectID": "python_code/globalNewton.html#tests-for-globalized-newton-method-backtracking-with-conventional-armijo-rule",
    "href": "python_code/globalNewton.html#tests-for-globalized-newton-method-backtracking-with-conventional-armijo-rule",
    "title": "MaxEnt",
    "section": "Tests for globalized Newton method (backtracking with conventional Armijo rule):",
    "text": "Tests for globalized Newton method (backtracking with conventional Armijo rule):\nTO-DO: - More thorough hyper-parameter tuning (rho, p, beta, s)\n\nmu = np.ones(x0.shape[0]) / x0.shape[0] # set uniform prior mu\nm, n = A.shape\nlmda=16e-17\n\nx, y, gaps, armijo_steps, Gnorms = GlobalNewton(A, mu, b, lmda, eps=10e-5, max_iters=10000, verbose=True, p=1)\n\n----------\nIteration: 1\nPrimal value: 1097224357798948.6\nDual value: 2.2204460492503128e-16\nIter: 1, Dual Jacobian Norm: 0.592546871138194, Primal-Dual Gap: 1097224357798948.6\n----------\nIteration: 1001\nPrimal value: 99491197448.95615\nDual value: -0.22726906711905315\nIter: 1001, Dual Jacobian Norm: 0.005642444787819557, Primal-Dual Gap: 99491197448.72888\nInsuff decrease: Armijo checked 2 times\ndk norm 0.005642444787819557\ndot prod -3.183718318359209e-05\n----------\nIteration: 2001\nPrimal value: 77747867620.13237\nDual value: -0.22740225879889575\nIter: 2001, Dual Jacobian Norm: 0.004987917164345155, Primal-Dual Gap: 77747867619.90497\nInsuff decrease: Armijo checked 2 times\ndk norm 0.004987917164345155\ndot prod -2.487931763836901e-05\n----------\nIteration: 3001\nPrimal value: 36486650727.63614\nDual value: -0.2275136861035023\nIter: 3001, Dual Jacobian Norm: 0.0034169764753025364, Primal-Dual Gap: 36486650727.40862\n----------\nIteration: 4001\nPrimal value: 29342873315.686836\nDual value: -0.22760410027214562\nIter: 4001, Dual Jacobian Norm: 0.0030642649136370737, Primal-Dual Gap: 29342873315.459232\n----------\nIteration: 5001\nPrimal value: 40374318500.24589\nDual value: -0.2276767091755807\nIter: 5001, Dual Jacobian Norm: 0.0035944098152556865, Primal-Dual Gap: 40374318500.01821\nInsuff decrease: Armijo checked 2 times\ndk norm 0.0035944098152556865\ndot prod -1.2919781920006419e-05\n----------\nIteration: 6001\nPrimal value: 33576790115.879444\nDual value: -0.2277369042387256\nIter: 6001, Dual Jacobian Norm: 0.0032778915230692474, Primal-Dual Gap: 33576790115.651707\nInsuff decrease: Armijo checked 2 times\ndk norm 0.0032778915230692474\ndot prod -1.074457283700923e-05\n----------\nIteration: 7001\nPrimal value: 16857025698.227873\nDual value: -0.2277874081525817\nIter: 7001, Dual Jacobian Norm: 0.0023225520927119135, Primal-Dual Gap: 16857025698.000086\n----------\nIteration: 8001\nPrimal value: 14337331604.877762\nDual value: -0.22783044937325214\nIter: 8001, Dual Jacobian Norm: 0.0021419491388659645, Primal-Dual Gap: 14337331604.649931\n----------\nIteration: 9001\nPrimal value: 12324398400.218485\nDual value: -0.22786765538547793\nIter: 9001, Dual Jacobian Norm: 0.0019859021849017965, Primal-Dual Gap: 12324398399.990618\n----------\nSummary:\nTotal iters: 10000\nArmijo steps: 5048\nJacobian Norm: 0.0023976458744187326\nPrimal-Dual Gap: 6442485256.731591\nPrimal value: 17964705434.966583\nDual value: -0.22789954620057928\nRMS(x-x0): 0.00045865351951943827\nRMS(Ax-b): 0.0003390783313370732\n\n\n\nlog_gaps = np.log(gaps)\niters = np.arange(1,10001).reshape(10000,1)\n# Create subplots\nfig, axs = plt.subplots(3, 1, figsize=(8, 16))\n\n# Plot 1: log(Primal Dual Gap)\naxs[0].plot(iters, log_gaps)\naxs[0].plot(armijo_steps, [log_gaps[i] for i in armijo_steps], 'ro', label='gradient step taken')\naxs[0].set_title('log(Primal Dual Gap)')\naxs[0].set_xlabel('iteration')\naxs[0].set_ylim(20, 29)\naxs[0].legend()\n\n# Plot 2: log(Norm of dual Jacobian)\naxs[1].plot(np.log(Gnorms))\naxs[1].set_title('log(Norm of dual Jacobian)')\naxs[1].set_xlabel('iteration')\naxs[1].set_ylim(-9, -4)\n\n# Plot 3: Comparison plot\naxs[2].plot(x0, label=\"ground truth\")\naxs[2].plot(x, label=f\"{lmda}\")\naxs[2].plot(mu, label=\"prior\")\naxs[2].legend()\naxs[2].grid()\n\n# Adjust layout\nplt.tight_layout()\n\n# Show plot\nplt.show()"
  },
  {
    "objectID": "python_code/globalNewton.html#tests-using-the-basic-local-newton-method",
    "href": "python_code/globalNewton.html#tests-using-the-basic-local-newton-method",
    "title": "MaxEnt",
    "section": "Tests using the basic local Newton Method",
    "text": "Tests using the basic local Newton Method\n\nmu = np.ones(x0.shape[0]) / x0.shape[0] # set uniform prior mu\nm, n = A.shape\n\n#lmds = [3e-16]\nlmds = [3e-16, 1e-15, 1e-10, 1e-5, 1e-2]      \n# lmds = [1e-1, 1e-2, 1e-4]   \n# lmds = [1e-1] \nplt.plot(x0, label=\"ground truth\")\nfor lmda in lmds:\n    x, y, _ = LocalNewton(A, mu, b, lmda=lmda, eps=1e-6, max_iters=100, verbose=False)\n    plt.plot(x, label=f\"{lmda}\")\n    print(f'Newton Sol (lambda={lmda}) RMS(x-x0): {np.linalg.norm(x0 - x) / np.sqrt(n)}')\n    print(f'Newton Sol (lambda={lmda}) RMS(Ax-b): {np.linalg.norm(A @ x - b) / np.sqrt(m)}')\n    print('-' * 10)\nx_ = np.linalg.lstsq(A, b, rcond=None)\nplt.plot(x_[0], label='OLS Sol')\nplt.plot(mu, label=\"prior\")\nplt.legend()\nplt.grid()\n\nprint(f'OLS Sol RMS(x-x0): {np.linalg.norm(x0 - x_[0]) / np.sqrt(n)}')\nprint(f'OLS Sol RMS(Ax-b): {np.linalg.norm(A @ x_[0] - b) / np.sqrt(m)}')\n\nNewton Sol (lambda=3e-16) RMS(x-x0): 0.00047092935506106446\nNewton Sol (lambda=3e-16) RMS(Ax-b): 0.0010554123592560406\n----------\nNewton Sol (lambda=1e-15) RMS(x-x0): 0.0004943197220328498\nNewton Sol (lambda=1e-15) RMS(Ax-b): 0.0011605674771236797\n----------\nNewton Sol (lambda=1e-10) RMS(x-x0): 0.0008191634890288051\nNewton Sol (lambda=1e-10) RMS(Ax-b): 0.0016598232831281253\n----------\nNewton Sol (lambda=1e-05) RMS(x-x0): 0.001521669066519315\nNewton Sol (lambda=1e-05) RMS(Ax-b): 0.022980629196860024\n----------\nNewton Sol (lambda=0.01) RMS(x-x0): 0.0018607672897144275\nNewton Sol (lambda=0.01) RMS(Ax-b): 0.06633638326771332\n----------\nOLS Sol RMS(x-x0): 0.0006539727444728927\nOLS Sol RMS(Ax-b): 2.168002603497676e-15"
  },
  {
    "objectID": "test/scratch.html",
    "href": "test/scratch.html",
    "title": "MaxEnt",
    "section": "",
    "text": "import Pkg; Pkg.activate(\"..\"); Pkg.instantiate()\nusing Revise\nusing NLPModels\nusing KLLS\nusing JSOSolvers\n\n  Activating project at `~/.julia/dev/KLLS`\n\n\n\nm, n = 2, 3\nA = randn(m, n)\np = (v=rand(n); v=v/sum(v))\nb = A*p + 0.1*randn(m)\nλ = 1e-1\ndata = KLLSData(A, b, λ=λ)\nnlp = KLLSModel(data);\n\n\nstats = trunk(nlp, verbose=1)\n\n┌ Info:   iter      f(x)         π         Δ     ratio   inner      bk         cgstatus  \n└ @ JSOSolvers /Users/mpf/.julia/packages/JSOSolvers/zfm1c/src/trunk.jl:200\n┌ Info:      0  -1.1e-16   2.3e-01         -         -       -       -                -\n└ @ JSOSolvers /Users/mpf/.julia/packages/JSOSolvers/zfm1c/src/trunk.jl:205\n┌ Info:      1  -1.3e-01   3.4e-01   1.0e+00   1.0e+00       0       0  on trust-region boundary\n└ @ JSOSolvers /Users/mpf/.julia/packages/JSOSolvers/zfm1c/src/trunk.jl:368\n┌ Info:      2  -2.3e-01   3.1e-02   1.5e+00   9.8e-01       0       0  solution good enough given atol and rtol\n└ @ JSOSolvers /Users/mpf/.julia/packages/JSOSolvers/zfm1c/src/trunk.jl:368\n┌ Info:      3  -2.3e-01   2.3e-04   1.8e+00   1.0e+00       0       0  solution good enough given atol and rtol\n└ @ JSOSolvers /Users/mpf/.julia/packages/JSOSolvers/zfm1c/src/trunk.jl:368\n┌ Info:      4  -2.3e-01   6.0e-09   1.8e+00   1.0e+00       0       0  solution good enough given atol and rtol\n└ @ JSOSolvers /Users/mpf/.julia/packages/JSOSolvers/zfm1c/src/trunk.jl:368\n┌ Info:      4  -2.3e-01   6.0e-09   1.8e+00\n└ @ JSOSolvers /Users/mpf/.julia/packages/JSOSolvers/zfm1c/src/trunk.jl:402\n\n\n\"Execution stats: first-order stationary\""
  },
  {
    "objectID": "quarto/index.html",
    "href": "quarto/index.html",
    "title": "Maximum Entropy Linear Inversion Project",
    "section": "",
    "text": "synthetic data"
  },
  {
    "objectID": "quarto/index.html#test-sets",
    "href": "quarto/index.html#test-sets",
    "title": "Maximum Entropy Linear Inversion Project",
    "section": "",
    "text": "synthetic data"
  },
  {
    "objectID": "quarto/synthetic-data.html",
    "href": "quarto/synthetic-data.html",
    "title": "KL-Regularized Least Squares",
    "section": "",
    "text": "Import packages\nusing Revise\nusing KLLS\nusing NPZ\nusing Plots\nusing Printf\nusing UnPack\nusing LinearAlgebra"
  },
  {
    "objectID": "quarto/synthetic-data.html#data-sets",
    "href": "quarto/synthetic-data.html#data-sets",
    "title": "KL-Regularized Least Squares",
    "section": "Data sets",
    "text": "Data sets\nThere are three data sets available:\n\nPhysicsData.npz\nsynthetic_data.npz\nTobias_data.npz\n\n\nQuestions\n\nThe Tobias_data.npz file doesn’t seem to be any different from synthetic_data.npz. Are these geniunely different data sets?\nWhat is the meaning of the b vector in synthetic_data.npz?\nHow do we normalize the data in synthetic_data.npz?\n\n\nPhysicsData:\n\ndataPhy = npzread(\"../data/PhysicsData.npz\", [\"A\", \"b\", \"x0\"])\nprobPhy = KLLSData(dataPhy[\"A\"], dataPhy[\"b\"], name=\"Physics\")\n\nKLLS data: Physics\nm   =    50   norm(b)    =  6.18e+00\nn   =   400   sum(b)     =  8.65e+00\n\n\nCheck normalization:\n\nsum(dataPhy[\"x0\"])\n\n1.0\n\n\n\n\nSynthetic data:\nFor this dataset, the ground truth seems to be named x instead of x0, as in the other data sets. We’ll rename it for consistency:\n\ndataSyn = npzread(\"../data/synthetic_data.npz\", [\"A\", \"b\", \"x\"])\ndataSyn[\"x0\"] = dataSyn[\"x\"] # convenient\nprobSyn = KLLSData(dataSyn[\"A\"], dataSyn[\"b\"], name=\"Synthetic Data\")\n\nKLLS data: Synthetic Data\nm   =   201   norm(b)    =  2.93e+01\nn   =   250   sum(b)     =  3.25e+02\n\n\nCheck normalization:\n\nsum(dataSyn[\"x0\"])\n\n6.3280697f0\n\n\n\n\n\n\n\n\nNormalization\n\n\n\nThe synthetic data doesn’t appear to be normalized. As agreed, we normalize the data such that the first element of the right-hand side vector \\(b\\) is 1:\n\nprobSyn.b ./= probSyn.b[1]\nprobSyn.A ./= probSyn.b[1]\ndataSyn[\"x0\"] ./= probSyn.b[1]\n\nQuestions:\n\nNote that this normalization still doesn’t put x0 into the simplex:\n\n\nsum(dataSyn[\"x0\"])\n\n6.3280697f0\n\n\n\nWhy is x0 for this test set stored as single precision float? (See “f” in line above.)\n\n\n\nHere are the signals to be recovered:\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecovery\nRecover the signals as the solution of the KL-regularized least squares problem \\[\n\\min_p \\frac{1}{2} \\|Ap - b\\|^2 + \\lambda \\sum_{i=1}^n p_i \\log(p_i)\n\\]\n\n\nImplement test functions\nlogrange(start, stop, length) = exp10.(range(start, stop=stop, length=length))\nsolve_range(prob, λs) = map(λs) do λ\n        prob.λ = λ\n        p, y, stats = newtoncg(prob)\n        (λ=λ, p=p, iters=stats.iter, solve_time=stats.elapsed_time)\nend\nfunction plot_results(stats, x0; title=\"Recovered distributions\")\n    lab = hcat([@sprintf(\"λ = %6.0e\", λ) for λ in getfield.(stats, :λ)]...)\n    default(lw=2, title=title)\n    plot(getfield.(stats, :p), label=lab)\n    plot!(x0, label=\"ground truth\")\nend\n\n\n\nPhysics data\nSolve the problem for a range of logarithmically spaced regularization parameters \\(\\lambda\\) between \\(10^{-16}\\) and \\(10^{-5}\\).\n\nstatsPhy = solve_range(probPhy, logrange(-16, -5, 3))\nplot_results(statsPhy, dataPhy[\"x0\"])\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReport results for smallest value of \\(\\lambda\\):\n\n\nImplement report function\nfunction report_errors(stats, data)\n  @unpack A, b, x0 = data\n  p = stats.p\n  m, n = size(A)\n  @printf(\"%20s: %11.2e\\n\", \"rms(p-x0)\", norm(p - x0)/√n)\n  @printf(\"%20s: %11.2e\\n\", \"rms(Ap-b)\", norm(A*p - b)/√m)\n  @printf(\"%20s: %11.2e\\n\", \"rms(Ax0-b)\", norm(A*x0 - b)/√m)\n  @printf(\"%20s: %11f\\n\", \"Solve time (sec)\", stats.solve_time)\nend\n\n\n\nreport_errors(statsPhy[1], dataPhy)\n\n           rms(p-x0):    4.57e-04\n           rms(Ap-b):    1.27e-08\n          rms(Ax0-b):    2.55e-16\n    Solve time (sec):    0.014391\n\n\n\n\nSynthetic data\nHere we try to recover the solution from the synthetic data set. But as we see here, we get garbage:\n\np, y, statsSyn = newtoncg(probSyn)\nplot([p, dataSyn[\"x0\"]], label=[\"Recovered\" \"Ground truth\"])\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIt seems that the vector b in that data set isn’t correct. What is b the meaning of b in this data set?\n\n\nAs a test, we construct b from the ground truth (forcibly normalized) x0:\n\nlet\n  @unpack A, x0 = dataSyn\n  x0 = x0/sum(x0)\n  b = A*x0\n  p, y, stats = newtoncg(KLLSData(A, b))\n  plot([p, x0], label=[\"Recovered\" \"Ground truth\"])\nend"
  },
  {
    "objectID": "quarto/preconditioning.html",
    "href": "quarto/preconditioning.html",
    "title": "Preconditioning",
    "section": "",
    "text": "Each iteration of the preconditioned Newton CG method approximately solves the trust-region subproblem \\[\n\\min_{s} \\{\\tfrac{1}{2} \\langle s, H_k s \\rangle + \\langle \\nabla f_k,s  \\rangle \\mid \\|s\\|_M \\leq \\Delta\\}\n\\] where \\(\\nabla f_k\\) and \\(H_k\\) are the current gradient and Hessian, and \\(M\\approx H\\) is a positive definite preconditioner. For the KL regularized LS problem, \\[\nH_k = A S_k A^T + \\lambda I, \\quad S_k:= X_k - x_k x_k^T\n\\] where \\(x_k\\) is the current primal iterate and \\(X_k=\\operatorname{Diag}(x_k)\\).\nWe consider these preconditioners:\n\nDiagonal, constant: \\(M = \\operatorname{Diag}(AA^T) + \\lambda I\\)\nDiagonal, variable: \\(M = \\operatorname{Diag}(AS_kA^T) + \\lambda I\\)\nCholesky, constant: \\(M = AA^T + \\lambda I\\)\n\nThe first two preconditioners have complexity \\(O(mn)\\) to compute and \\(O(m)\\) to apply. The third has cubic complexity, but that cost can be amortized over the Newton iterations.\n\n\nCode\nusing LinearAlgebra\nusing KLLS\nimport KLLS: Preconditioner\n\n\n```julia"
  }
]